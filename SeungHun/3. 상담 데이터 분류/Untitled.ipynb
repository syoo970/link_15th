{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "fp = codecs.open('yang.txt', 'r')\n",
    "lines = fp.readlines()  # 총 200줄의 내용\n",
    "# print(len(lines))  # 한줄 한줄이 요구사항이고 그에 따른 y도 200개\n",
    "# print(lines)\n",
    "twitter = Twitter() # Twitter는 Okt로 바뀜\n",
    "word_dic = {}\n",
    "fp.close()\n",
    "\n",
    "for line in lines:\n",
    "    malist = twitter.pos(line)\n",
    "    for word in malist:\n",
    "        if word[1] in ['Noun', 'Adjective', 'Verb', 'Adverb']:\n",
    "            if not (word[0] in word_dic):\n",
    "                word_dic[word[0]] = 0\n",
    "            word_dic[word[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배송 (99) 배달 (36) 주문 (33) 변경 (31) 되나요 (22) 가방 (21) 오늘 (20) 주 (17) 해서 (16) 했는데 (15) 요 (15) 신청 (15) 취소 (15) 우유 (14) 하루 (14) 해주세요 (13) 제 (13) 개 (13) 도시락 (13) 두유 (12) 언제 (12) 문의 (12) 좀 (11) 수 (11) 다이어트 (11) 혹시 (10) 상품 (10) 부탁드립니다 (9) 어떻게 (9) 건가 (9) 분유 (9) 싶습니다 (9) 가요 (9) 식품 (9) 시작 (9) 시 (9) 제품 (9) 받아 (9) 확인 (8) 주소 (8) 먹어도 (8) 메뉴 (8) 내일 (7) 시간 (7) 알 (7) 있나요 (7) 월 (7) 때 (7) 번 (7) 합니다 (7) 배송지 (7) 개월 (7) 가능한가요 (7) 날짜 (7) 안 (7) 하는데 (6) 아기 (6) 세트 (6) 구입 (6) 회사 (6) 추가 (6) 프로그램 (6) 구매 (6) 된 (6) 같이 (6) 날 (6) 너무 (6) 드립니다 (6) 잘 (6) 그럼 (6) 다 (6) 하면 (6) 왜 (5) 꼭 (5) 거 (5) 택배 (5) 것 (5) 미 (5) 가능할까 (5) 앞 (5) 를 (5) 정기 (5) 할 (5) 결제 (5) 커피 (5) 현관 (5) 더 (5) 했습니다 (5) 그냥 (5) 되는건 (5) 이번 (5) 한번 (5) 처음 (5) 월요일 (5) 일 (5) 건지 (4) 오는 (4) 없이 (4) 왔는데 (4) 집 (4) 총 (4) 같은 (4) 안녕하세요 (4) 하려면 (4) 쇼핑몰 (4) 하나요 (4) 체험 (4) 요청 (4) 뒤 (4) 때문 (4) 보 (4) 다음 (4) 부터 (4) 초기 (4) 해 (4) 도착 (4) 종료 (4) 식단 (4) 는걸 (4) 지방 (4) 있을까요 (4) 두부 (4) 출근 (3) 이후 (3) 사람 (3) 이전 (3) 이벤트 (3) 어디 (3) 지역 (3) 안되나요 (3) 부탁드려요 (3) 왔습니다 (3) 기존 (3) 왔어요 (3) 어제 (3) 무료 (3) 샘플 (3) 받고 (3) 지금 (3) 받게 (3) 이미 (3) 해야 (3) 된건 (3) 진행 (3) 중 (3) 문자 (3) 홈페이지 (3) 없나요 (3) 공휴일 (3) 방금 (3) 했는데요 (3) 입니다 (3) 만 (3) 설탕 (3) 환불 (3) 감량 (3) 아래 (3) 받는 (3) 매일 (3) 다음주 (3) 받을 (3) 기간 (3) 일회용 (3) 달 (3) 부분 (3) 빠른 (3) 오나요 (3) 하여 (3) 기분 (3) 같아요 (3) 될까 (3) 같은데요 (3) 현재 (3) 하려고 (3) 외부 (3) 하나 (3) 휴무 (3) 화요일 (3) 담당자 (3) 이메일 (3) 수금 (3) 싶어요 (3) 경우 (3) 격일 (3) 마트 (3) 아이 (3) 인형 (3) 구 (3) 이제 (2) 받아줄 (2) 없고 (2) 아침 (2) 아직도 (2) 전 (2) 없어서요 (2) 가지 (2) 금일 (2) 요망 (2) 아니라고 (2) 보내주시면 (2) 경기도 (2) 중단 (2) 요거트 (2) 일자 (2) 원합니다 (2) 주치 (2) 주기 (2) 금 (2) 화 (2) 않고 (2) 위 (2) 주세요 (2) 저 (2) 와요 (2) 동안 (2) 또 (2) 역 (2) 머신 (2) 종류 (2) 받으면 (2) 해야하나요 (2) 공동 (2) 비밀번호 (2) 카페라떼 (2) 먹었는데 (2) 저희 (2) 했고 (2) 보니 (2) 함량 (2) 있습니다 (2) 다른 (2) 대체 (2) 아직 (2) 않았습니다 (2) 사서 (2) 물질 (2) 애기 (2) 입력 (2) 언니 (2) 한다고 (2) 있는데 (2) 오류 (2) 수정 (2) 안됩니다 (2) 곳 (2) 드려요 (2) 마지막 (2) 이마트 (2) 떡 (2) 먹는데 (2) 하고싶은데 (2) 아까 (2) 했던 (2) 아니면 (2) 걸어놓으면 (2) 분 (2) 남은 (2) 자꾸 (2) 겹 (2) 있으면 (2) 어디서 (2) 연락 (2) 가능한 (2) 받나요 (2) 일정 (2) 가입 (2) 그동안 (2) 건강 (2) 먹었습니다 (2) 제대로 (2) 유용하게 (2) 알기 (2) 론 (2) 상태 (2) 같은데 (2) 못 (2) 그 (2) 쭉 (2) 샐러드 (2) 하지 (2) 온거 (2) 사은 (2) 먹고 (2) 있는데요 (2) 휴가 (2) 안해 (2) 지정 (2) 건너 (2) 띄고 (2) 하고 (2) 휴일 (2) 몰에서 (2) 동일한 (2) 할께요 (2) 라서 (2) 매우 (2) 죄송해요 (2) 음식 (2) 왔네요 (2) 며칠 (2) 불만 (2) 딸기 (2) 바나나 (2) 양배추 (2) 알림 (2) 답변 (2) 수요일 (2) 캡슐 (2) 팩 (2) 유통 (2) 기한 (2) 말씀 (2) 달동 (2) 후 (2) 단계 (2) 있는 (2) 나눠서 (2) 없었습니다 (2) 일주일 (2) 약정 (2) 가장 (2) 살 (2) 알러지 (2) 대형 (2) 판매 (2) 이렇게 (2) 하려고하는데요 (2) 글 (2) 하였는데 (2) 없다고 (2) 게 (2) 오고 (2) 알려주세요 (2) 바로 (2) 토마스 (2) 주말 (2) 유모차 (2) 받았는데요 (2) 않았어요 (2) 아닌가요 (2) 반찬 (2) 일로 (2) 문건 (2) 킨 (1) 모르겠네요 (1) 해야하는데 (1) 되는거라면 (1) 그대로 (1) 버려지겠네요 (1) 안되어있습니다 (1) 제발 (1) 지켜주세요 (1) 늦어도 (1) 해봤더니 (1) 당첨 (1) 되었는데요 (1) 같던데 (1) 차주 (1) 발송 (1) 예정 (1) 데 (1) 적은 (1) 적 (1) 휴대폰 (1) 번호 (1) 보내주시는 (1) 해서요 (1) 맞게 (1) 돼요 (1) 로봇청소기 (1) 학생 (1) 책상 (1) 가능 (1) 이천시 (1) 가능한지 (1) 오프라인 (1) 처 (1) 의합 (1) 니 (1) 해당 (1) 되었습니다 (1) 침대 (1) 연기 (1) 해주세오 (1) 짜리 (1) 스케줄 (1) 목 (1) 로 (1) 놔두지 (1) 하실 (1) 세탁기 (1) 놓고 (1) 가 (1) 안오나요 (1) 건 (1) 한 (1) 싶은건데 (1) 되었네요 (1) 하우 (1) 온라인 (1) 중복 (1) 되었는데 (1) 완료 (1) 에러 (1) 났어요 (1) 카드 (1) 되었다고 (1) 뭐 (1) 있어요 (1) 끝나나요 (1) 있어서요 (1) 했어요 (1) 되겠죠 (1) 목동 (1) 받기로 (1) 애가 (1) 먹던것보다 (1) 향 (1) 진해서 (1) 약한 (1) 하길 (1) 희망 (1) 먹여도 (1) 성분 (1) 과 (1) 최근 (1) 속성 (1) 중요한데 (1) 되니 (1) 의욕 (1) 사라지네요 (1) 되지 (1) 순두부 (1) 주는데 (1) 나왔습니다 (1) 교환 (1) 또한 (1) 하려는데 (1) 안되요 (1) 월계동 (1) 나옵니다 (1) 집중 (1) 전자렌지 (1) 없는데 (1) 차갑고 (1) 맛 (1) 떨어져도 (1) 섭취 (1) 되긴 (1) 하는거죠 (1) 라떼 (1) 당뇨 (1) 환자 (1) 괜찮은 (1) 동생 (1) 먹이려니 (1) 나와서요 (1) 어찌 (1) 하다가 (1) 난 (1) 예전 (1) 주일 (1) 미뤄서 (1) 받았으면 (1) 미룰수있나요 (1) 여긴 (1) 서울 (1) 창 (1) 동점 (1) 떡볶이 (1) 점심 (1) 통과 (1) 머리카락 (1) 나왔어요 (1) 일요일 (1) 제주도 (1) 여행 (1) 일주 (1) 일치 (1) 부경 (1) 김동수 (1) 죄송하지만 (1) 내주신다 (1) 낼 (1) 수욜 (1) 토욜 (1) 담주에 (1) 주셔도 (1) 감사히 (1) 받겠습니다 (1) 하며 (1) 먹을거라고 (1) 말씀드렸었는데 (1) 된가요 (1) 식 (1) 받았습니다 (1) 기대했던것과 (1) 달라 (1) 받던지 (1) 받고자 (1) 원해요 (1) 개발 (1) 계획 (1) 퀄리티 (1) 좋아 (1) 만족스러워 (1) 꾸준히 (1) 먹고싶은데 (1) 가짓수 (1) 적어 (1) 치니 (1) 아쉽네요 (1) 정해졌는데 (1) 받고싶어요 (1) 텀 (1) 생겨서요 (1) 꺼내놓기 (1) 힘들어서 (1) 종이가방 (1) 가능하다해 (1) 내 (1) 구리시 (1) 호로 (1) 아파트 (1) 비번 (1) 없던데 (1) 먹일려고 (1) 딱 (1) 준비 (1) 식물성 (1) 유산균 (1) 쌀 (1) 은색 (1) 들어가있습니다 (1) 색 (1) 똑같아서 (1) 뚜껑 (1) 열어 (1) 버렸습니다 (1) 말씀드려야하나요 (1) 됐어요 (1) 신랑 (1) 이름 (1) 시켰는데 (1) 카톡 (1) 가능하게 (1) 편식 (1) 심하고 (1) 낳고 (1) 갖춰 (1) 먹지 (1) 끝났는데 (1) 더러워요 (1) 사용 (1) 있을 (1) 좋네요 (1) 여유 (1) 해주시고 (1) 주셨는데 (1) 제일 (1) 깨끗한 (1) 이어서 (1) 먹었어서 (1) 두번째 (1) 모든 (1) 오이 (1) 뺄 (1) 문 (1) 걸어 (1) 두면 (1) 하던 (1) 시키면 (1) 주는 (1) 되었나요 (1) 받다가 (1) 받았어요 (1) 왔구요 (1) 원래 (1) 연달 (1) 서 (1) 맞는거 (1) 다시 (1) 되는 (1) 품주는것도 (1) 인해 (1) 구체 (1) 싶은데요 (1) 해지 (1) 되는걸 (1) 볼수있을까요 (1) 문후 (1) 되는거 (1) 맞나요 (1) 두 (1) 받을께요 (1) 용기 (1) 깨져있습니다 (1) 날카롭게 (1) 되있고 (1) 조각 (1) 먹게 (1) 될 (1) 위험 (1) 있었습니다 (1) 이랬다 (1) 저랬다 (1) 껄 (1) 보기 (1) 하고요 (1) 했구요 (1) 드린 (1) 바 (1) 팀 (1) 혹은 (1) 오늘아침 (1) 맘대로 (1) 바뀌어져있네요 (1) 월화 (1) 목금 (1) 해놓았는데 (1) 걸 (1) 다시주세요 (1) 돈 (1) 주고 (1) 산건데 (1) 별로 (1) 전달 (1) 해주시길 (1) 와서 (1) 세로 (1) 세워져있는 (1) 바람 (1) 샜음 (1) 진짜 (1) 짜증나네요 (1) 의 (1) 알고싶어요 (1) 거보 (1) 빠져 (1) 저지방우유 (1) 작일 (1) 따로 (1) 정도 (1) 걸린다 (1) 얘기 (1) 적혀있었는데 (1) 안되어서요 (1) 치면 (1) 보관 (1) 어려울 (1) 되는지 (1) 임시 (1) 생각 (1) 했네요 (1) 그렇다면 (1) 나온 (1) 콩나물 (1) 밥 (1) 심각하게 (1) 설익었습니다 (1) 생쌀 (1) 먹는줄 (1) 알았어요 (1) 익혀주세요 (1) 지점 (1) 및 (1) 안내 (1) 고지 (1) 않음 (1) 서비스 (1) 중간 (1) 미뤄지는거죠 (1) 유지 (1) 이용 (1) 표 (1) 간식 (1) 제공 (1) 집앞 (1) 야구르트 (1) 백안 (1) 넣어 (1) 두시 (1) 가지고가셨네요 (1) 이건 (1) 무슨 (1) 혈행 (1) 개선 (1) 효과 (1) 좋은 (1) 맛있나요 (1) 카페 (1) 망고 (1) 사과 (1) 쥬스 (1) 오기 (1) 사항 (1) 톡 (1) 받아서요 (1) 부탁드리겠습니다 (1) 받았는데 (1) 오는거죠 (1) 평일 (1) 안되는건 (1) 토요일 (1) 쯤 (1) 오는지 (1) 오긴 (1) 금주 (1) 인하여 (1) 됐는데 (1) 상의 (1) 착오 (1) 아닌가 (1) 있는대요 (1) 소바 (1) 행사 (1) 용이 (1) 받고있고 (1) 오전 (1) 먹이고있는데 (1) 먹으며 (1) 양 (1) 늘려야 (1) 할지 (1) 시기 (1) 연휴 (1) 욜 (1) 보는데 (1) 문제 (1) 없을까요 (1) 달이 (1) 먹으라고 (1) 하셨는데 (1) 알아서 (1) 바꿔서 (1) 놓은 (1) 알레르기 (1) 첨 (1) 이해 (1) 잘안가서 (1) 그러는데 (1) 먹을 (1) 먹으라는 (1) 몇번 (1) 나눠 (1) 서라도 (1) 먹게하라는 (1) 몰라서요 (1) 그래서 (1) 먹는걸 (1) 넘어간다는 (1) 저녁 (1) 치킨 (1) 먹으려고 (1) 유기농 (1) 계란 (1) 해요 (1) 쉐이크 (1) 품 (1) 안오네요 (1) 보름 (1) 보내준다고 (1) 했던것 (1) 이틀 (1) 대문 (1) 대부분 (1) 누락 (1) 정지 (1) 목요일 (1) 한꺼 (1) 남자 (1) 있어서 (1) 먹이려는데 (1) 어린이집 (1) 보내야 (1) 푸딩 (1) 살아이가 (1) 제조 (1) 신선하지 (1) 토핑 (1) 소스 (1) 개옴 (1) 할려고 (1) 인천 (1) 아님 (1) 받는거예요 (1) 몇개 (1) 배 (1) 송료 (1) 브로콜리 (1) 음료 (1) 받는거 (1) 선택 (1) 받는거로 (1) 하려고하는데 (1) 더라구요 (1) 그러면 (1) 해주시나요 (1) 그람 (1) 사와서 (1) 뜯었는데 (1) 터져있네요 (1) 찝찝해서 (1) 먹지도 (1) 하겠구요 (1) 이런 (1) 남깁니다 (1) 폰 (1) 남기기 (1) 어렵더군요 (1) 얼마 (1) 청소기 (1) 볼 (1) 있을지 (1) 궁금해서 (1) 하는거 (1) 할려면 (1) 여기 (1) 해도 (1) 그날 (1) 연장 (1) 스케쥴 (1) 작성 (1) 된게 (1) 음 (1) 뜨네요 (1) 날로 (1) 했었는데 (1) 지나면 (1) 자동 (1) 상세 (1) 되는데요 (1) 원룸 (1) 거주 (1) 하는데요 (1) 건물 (1) 두고 (1) 방문 (1) 옮기고 (1) 기사 (1) 초인종 (1) 누르면 (1) 여 (1) 방법 (1) 바꾸고 (1) 늦은 (1) 새벽 (1) 상관없어요 (1) 상자 (1) 첫 (1) 전날 (1) 미리 (1) 당일 (1) 싶은데 (1) 사이트 (1) 접속 (1) 하니 (1) 계속 (1) 나서요 (1) 예정일 (1) 뜹니다 (1) 답답 (1) 좋아하는 (1) 할수 (1) 사촌언니 (1) 네사 (1) 얻어 (1) 좋아해서 (1) 끌어안고 (1) 타고 (1) 나갔다가 (1) 떨어뜨린것 (1) 유유 (1) 평균 (1) 나트륨 (1) 네 (1) 감사해요 (1) 기다릴테니 (1) 관련 (1) 부서 (1) 하셔서 (1) 부탁드릴게요 (1) 차라리 (1) 파는거면 (1) 당장 (1) 사겠는데 (1) 그것 (1) 아니라서 (1) 하기가 (1) 어렵네요 (1) 파란 (1) 안고다니구 (1) 탈때 (1) 쥐어주니 (1) 타길래 (1) 들고 (1) 나갔다 (1) 그만 (1) 그래도 (1) 통해 (1) 먹고있는데 (1) 애용 (1) 할테니 (1) 알아봐주세요 (1) 하게되 (1) 면 (1) 맨날 (1) 노래 (1) 부르고 (1) 다닐게요 (1) 정말 (1) 감사합니다 (1) 박스 (1) 중지 (1) 오질 (1) 되는게 (1) 금요일 (1) 있거든요 (1) 안된다면 (1) 신규 (1) 지인 (1) 아이디 (1) 없으면 (1) 할인 (1) 사제품 (1) 프로 (1) 해주던데 (1) 이었습니다 (1) 포장 (1) 뜯고 (1) 찍은 (1) 사진 (1) 가끔 (1) 성의 (1) 담아 (1) 보낼 (1) 있더군요 (1) 먹다 (1) 싸준거 (1) 같아서 (1) 좋지 (1) 제작 (1) 과정 (1) 그럴수 (1) 있겠지만 (1) 신경 (1) 쓰셔야 (1) 할거 (1) 거부 (1) 바랍니다 (1) 잘못 (1) 봐두 (1) 최종 (1) 까지인가요 (1) 정확 (1) 국내 (1) 산 (1) 쓰나요 (1) 치즈 (1) 어떤 (1) 되네요 (1) 이 (1) 포함 (1) 말아주세요 (1) 요즘 (1) 자주 (1) 내용물 (1) 넘쳐있어요 (1) 짜증 (1) 이나 (1) \n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "keys = sorted(word_dic.items(), key=operator.itemgetter(1), reverse=True)\n",
    "# keys 글자와 빈도수를 묶는 튜플을 갖는 리스트\n",
    "for word, count in keys[:10000]:\n",
    "    print('{0} ({1}) '.format(word, count), end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ('배송', 99)\n",
      "2 ('배달', 36)\n",
      "3 ('주문', 33)\n",
      "4 ('변경', 31)\n",
      "5 ('되나요', 22)\n",
      "6 ('가방', 21)\n",
      "7 ('오늘', 20)\n",
      "8 ('주', 17)\n",
      "9 ('해서', 16)\n",
      "10 ('했는데', 15)\n",
      "11 ('요', 15)\n",
      "12 ('신청', 15)\n",
      "13 ('취소', 15)\n",
      "14 ('우유', 14)\n",
      "15 ('하루', 14)\n",
      "16 ('해주세요', 13)\n",
      "17 ('제', 13)\n",
      "18 ('개', 13)\n",
      "19 ('도시락', 13)\n",
      "20 ('두유', 12)\n",
      "21 ('언제', 12)\n",
      "22 ('문의', 12)\n",
      "23 ('좀', 11)\n",
      "24 ('수', 11)\n",
      "25 ('다이어트', 11)\n",
      "26 ('혹시', 10)\n",
      "27 ('상품', 10)\n",
      "28 ('부탁드립니다', 9)\n",
      "29 ('어떻게', 9)\n",
      "30 ('건가', 9)\n",
      "31 ('분유', 9)\n",
      "32 ('싶습니다', 9)\n",
      "33 ('가요', 9)\n",
      "34 ('식품', 9)\n",
      "35 ('시작', 9)\n",
      "36 ('시', 9)\n",
      "37 ('제품', 9)\n",
      "38 ('받아', 9)\n",
      "39 ('확인', 8)\n",
      "40 ('주소', 8)\n",
      "41 ('먹어도', 8)\n",
      "42 ('메뉴', 8)\n",
      "43 ('내일', 7)\n",
      "44 ('시간', 7)\n",
      "45 ('알', 7)\n",
      "46 ('있나요', 7)\n",
      "47 ('월', 7)\n",
      "48 ('때', 7)\n",
      "49 ('번', 7)\n",
      "50 ('합니다', 7)\n",
      "51 ('배송지', 7)\n",
      "52 ('개월', 7)\n",
      "53 ('가능한가요', 7)\n",
      "54 ('날짜', 7)\n",
      "55 ('안', 7)\n",
      "56 ('하는데', 6)\n",
      "57 ('아기', 6)\n",
      "58 ('세트', 6)\n",
      "59 ('구입', 6)\n",
      "60 ('회사', 6)\n",
      "61 ('추가', 6)\n",
      "62 ('프로그램', 6)\n",
      "63 ('구매', 6)\n",
      "64 ('된', 6)\n",
      "65 ('같이', 6)\n",
      "66 ('날', 6)\n",
      "67 ('너무', 6)\n",
      "68 ('드립니다', 6)\n",
      "69 ('잘', 6)\n",
      "70 ('그럼', 6)\n",
      "71 ('다', 6)\n",
      "72 ('하면', 6)\n",
      "73 ('왜', 5)\n",
      "74 ('꼭', 5)\n",
      "75 ('거', 5)\n",
      "76 ('택배', 5)\n",
      "77 ('것', 5)\n",
      "78 ('미', 5)\n",
      "79 ('가능할까', 5)\n",
      "80 ('앞', 5)\n",
      "81 ('를', 5)\n",
      "82 ('정기', 5)\n",
      "83 ('할', 5)\n",
      "84 ('결제', 5)\n",
      "85 ('커피', 5)\n",
      "86 ('현관', 5)\n",
      "87 ('더', 5)\n",
      "88 ('했습니다', 5)\n",
      "89 ('그냥', 5)\n",
      "90 ('되는건', 5)\n",
      "91 ('이번', 5)\n",
      "92 ('한번', 5)\n",
      "93 ('처음', 5)\n",
      "94 ('월요일', 5)\n",
      "95 ('일', 5)\n",
      "96 ('건지', 4)\n",
      "97 ('오는', 4)\n",
      "98 ('없이', 4)\n",
      "99 ('왔는데', 4)\n",
      "100 ('집', 4)\n",
      "101 ('총', 4)\n",
      "102 ('같은', 4)\n",
      "103 ('안녕하세요', 4)\n",
      "104 ('하려면', 4)\n",
      "105 ('쇼핑몰', 4)\n",
      "106 ('하나요', 4)\n",
      "107 ('체험', 4)\n",
      "108 ('요청', 4)\n",
      "109 ('뒤', 4)\n",
      "110 ('때문', 4)\n",
      "111 ('보', 4)\n",
      "112 ('다음', 4)\n",
      "113 ('부터', 4)\n",
      "114 ('초기', 4)\n",
      "115 ('해', 4)\n",
      "116 ('도착', 4)\n",
      "117 ('종료', 4)\n",
      "118 ('식단', 4)\n",
      "119 ('는걸', 4)\n",
      "120 ('지방', 4)\n",
      "121 ('있을까요', 4)\n",
      "122 ('두부', 4)\n",
      "123 ('출근', 3)\n",
      "124 ('이후', 3)\n",
      "125 ('사람', 3)\n",
      "126 ('이전', 3)\n",
      "127 ('이벤트', 3)\n",
      "128 ('어디', 3)\n",
      "129 ('지역', 3)\n",
      "130 ('안되나요', 3)\n",
      "131 ('부탁드려요', 3)\n",
      "132 ('왔습니다', 3)\n",
      "133 ('기존', 3)\n",
      "134 ('왔어요', 3)\n",
      "135 ('어제', 3)\n",
      "136 ('무료', 3)\n",
      "137 ('샘플', 3)\n",
      "138 ('받고', 3)\n",
      "139 ('지금', 3)\n",
      "140 ('받게', 3)\n",
      "141 ('이미', 3)\n",
      "142 ('해야', 3)\n",
      "143 ('된건', 3)\n",
      "144 ('진행', 3)\n",
      "145 ('중', 3)\n",
      "146 ('문자', 3)\n",
      "147 ('홈페이지', 3)\n",
      "148 ('없나요', 3)\n",
      "149 ('공휴일', 3)\n",
      "150 ('방금', 3)\n",
      "151 ('했는데요', 3)\n",
      "152 ('입니다', 3)\n",
      "153 ('만', 3)\n",
      "154 ('설탕', 3)\n",
      "155 ('환불', 3)\n",
      "156 ('감량', 3)\n",
      "157 ('아래', 3)\n",
      "158 ('받는', 3)\n",
      "159 ('매일', 3)\n",
      "160 ('다음주', 3)\n",
      "161 ('받을', 3)\n",
      "162 ('기간', 3)\n",
      "163 ('일회용', 3)\n",
      "164 ('달', 3)\n",
      "165 ('부분', 3)\n",
      "166 ('빠른', 3)\n",
      "167 ('오나요', 3)\n",
      "168 ('하여', 3)\n",
      "169 ('기분', 3)\n",
      "170 ('같아요', 3)\n",
      "171 ('될까', 3)\n",
      "172 ('같은데요', 3)\n",
      "173 ('현재', 3)\n",
      "174 ('하려고', 3)\n",
      "175 ('외부', 3)\n",
      "176 ('하나', 3)\n",
      "177 ('휴무', 3)\n",
      "178 ('화요일', 3)\n",
      "179 ('담당자', 3)\n",
      "180 ('이메일', 3)\n",
      "181 ('수금', 3)\n",
      "182 ('싶어요', 3)\n",
      "183 ('경우', 3)\n",
      "184 ('격일', 3)\n",
      "185 ('마트', 3)\n",
      "186 ('아이', 3)\n",
      "187 ('인형', 3)\n",
      "188 ('구', 3)\n",
      "189 ('이제', 2)\n",
      "190 ('받아줄', 2)\n",
      "191 ('없고', 2)\n",
      "192 ('아침', 2)\n",
      "193 ('아직도', 2)\n",
      "194 ('전', 2)\n",
      "195 ('없어서요', 2)\n",
      "196 ('가지', 2)\n",
      "197 ('금일', 2)\n",
      "198 ('요망', 2)\n",
      "199 ('아니라고', 2)\n",
      "200 ('보내주시면', 2)\n",
      "201 ('경기도', 2)\n",
      "202 ('중단', 2)\n",
      "203 ('요거트', 2)\n",
      "204 ('일자', 2)\n",
      "205 ('원합니다', 2)\n",
      "206 ('주치', 2)\n",
      "207 ('주기', 2)\n",
      "208 ('금', 2)\n",
      "209 ('화', 2)\n",
      "210 ('않고', 2)\n",
      "211 ('위', 2)\n",
      "212 ('주세요', 2)\n",
      "213 ('저', 2)\n",
      "214 ('와요', 2)\n",
      "215 ('동안', 2)\n",
      "216 ('또', 2)\n",
      "217 ('역', 2)\n",
      "218 ('머신', 2)\n",
      "219 ('종류', 2)\n",
      "220 ('받으면', 2)\n",
      "221 ('해야하나요', 2)\n",
      "222 ('공동', 2)\n",
      "223 ('비밀번호', 2)\n",
      "224 ('카페라떼', 2)\n",
      "225 ('먹었는데', 2)\n",
      "226 ('저희', 2)\n",
      "227 ('했고', 2)\n",
      "228 ('보니', 2)\n",
      "229 ('함량', 2)\n",
      "230 ('있습니다', 2)\n",
      "231 ('다른', 2)\n",
      "232 ('대체', 2)\n",
      "233 ('아직', 2)\n",
      "234 ('않았습니다', 2)\n",
      "235 ('사서', 2)\n",
      "236 ('물질', 2)\n",
      "237 ('애기', 2)\n",
      "238 ('입력', 2)\n",
      "239 ('언니', 2)\n",
      "240 ('한다고', 2)\n",
      "241 ('있는데', 2)\n",
      "242 ('오류', 2)\n",
      "243 ('수정', 2)\n",
      "244 ('안됩니다', 2)\n",
      "245 ('곳', 2)\n",
      "246 ('드려요', 2)\n",
      "247 ('마지막', 2)\n",
      "248 ('이마트', 2)\n",
      "249 ('떡', 2)\n",
      "250 ('먹는데', 2)\n",
      "251 ('하고싶은데', 2)\n",
      "252 ('아까', 2)\n",
      "253 ('했던', 2)\n",
      "254 ('아니면', 2)\n",
      "255 ('걸어놓으면', 2)\n",
      "256 ('분', 2)\n",
      "257 ('남은', 2)\n",
      "258 ('자꾸', 2)\n",
      "259 ('겹', 2)\n",
      "260 ('있으면', 2)\n",
      "261 ('어디서', 2)\n",
      "262 ('연락', 2)\n",
      "263 ('가능한', 2)\n",
      "264 ('받나요', 2)\n",
      "265 ('일정', 2)\n",
      "266 ('가입', 2)\n",
      "267 ('그동안', 2)\n",
      "268 ('건강', 2)\n",
      "269 ('먹었습니다', 2)\n",
      "270 ('제대로', 2)\n",
      "271 ('유용하게', 2)\n",
      "272 ('알기', 2)\n",
      "273 ('론', 2)\n",
      "274 ('상태', 2)\n",
      "275 ('같은데', 2)\n",
      "276 ('못', 2)\n",
      "277 ('그', 2)\n",
      "278 ('쭉', 2)\n",
      "279 ('샐러드', 2)\n",
      "280 ('하지', 2)\n",
      "281 ('온거', 2)\n",
      "282 ('사은', 2)\n",
      "283 ('먹고', 2)\n",
      "284 ('있는데요', 2)\n",
      "285 ('휴가', 2)\n",
      "286 ('안해', 2)\n",
      "287 ('지정', 2)\n",
      "288 ('건너', 2)\n",
      "289 ('띄고', 2)\n",
      "290 ('하고', 2)\n",
      "291 ('휴일', 2)\n",
      "292 ('몰에서', 2)\n",
      "293 ('동일한', 2)\n",
      "294 ('할께요', 2)\n",
      "295 ('라서', 2)\n",
      "296 ('매우', 2)\n",
      "297 ('죄송해요', 2)\n",
      "298 ('음식', 2)\n",
      "299 ('왔네요', 2)\n",
      "300 ('며칠', 2)\n",
      "301 ('불만', 2)\n",
      "302 ('딸기', 2)\n",
      "303 ('바나나', 2)\n",
      "304 ('양배추', 2)\n",
      "305 ('알림', 2)\n",
      "306 ('답변', 2)\n",
      "307 ('수요일', 2)\n",
      "308 ('캡슐', 2)\n",
      "309 ('팩', 2)\n",
      "310 ('유통', 2)\n",
      "311 ('기한', 2)\n",
      "312 ('말씀', 2)\n",
      "313 ('달동', 2)\n",
      "314 ('후', 2)\n",
      "315 ('단계', 2)\n",
      "316 ('있는', 2)\n",
      "317 ('나눠서', 2)\n",
      "318 ('없었습니다', 2)\n",
      "319 ('일주일', 2)\n",
      "320 ('약정', 2)\n",
      "321 ('가장', 2)\n",
      "322 ('살', 2)\n",
      "323 ('알러지', 2)\n",
      "324 ('대형', 2)\n",
      "325 ('판매', 2)\n",
      "326 ('이렇게', 2)\n",
      "327 ('하려고하는데요', 2)\n",
      "328 ('글', 2)\n",
      "329 ('하였는데', 2)\n",
      "330 ('없다고', 2)\n",
      "331 ('게', 2)\n",
      "332 ('오고', 2)\n",
      "333 ('알려주세요', 2)\n",
      "334 ('바로', 2)\n",
      "335 ('토마스', 2)\n",
      "336 ('주말', 2)\n",
      "337 ('유모차', 2)\n",
      "338 ('받았는데요', 2)\n",
      "339 ('않았어요', 2)\n",
      "340 ('아닌가요', 2)\n",
      "341 ('반찬', 2)\n",
      "342 ('일로', 2)\n",
      "343 ('문건', 2)\n",
      "344 ('킨', 1)\n",
      "345 ('모르겠네요', 1)\n",
      "346 ('해야하는데', 1)\n",
      "347 ('되는거라면', 1)\n",
      "348 ('그대로', 1)\n",
      "349 ('버려지겠네요', 1)\n",
      "350 ('안되어있습니다', 1)\n",
      "351 ('제발', 1)\n",
      "352 ('지켜주세요', 1)\n",
      "353 ('늦어도', 1)\n",
      "354 ('해봤더니', 1)\n",
      "355 ('당첨', 1)\n",
      "356 ('되었는데요', 1)\n",
      "357 ('같던데', 1)\n",
      "358 ('차주', 1)\n",
      "359 ('발송', 1)\n",
      "360 ('예정', 1)\n",
      "361 ('데', 1)\n",
      "362 ('적은', 1)\n",
      "363 ('적', 1)\n",
      "364 ('휴대폰', 1)\n",
      "365 ('번호', 1)\n",
      "366 ('보내주시는', 1)\n",
      "367 ('해서요', 1)\n",
      "368 ('맞게', 1)\n",
      "369 ('돼요', 1)\n",
      "370 ('로봇청소기', 1)\n",
      "371 ('학생', 1)\n",
      "372 ('책상', 1)\n",
      "373 ('가능', 1)\n",
      "374 ('이천시', 1)\n",
      "375 ('가능한지', 1)\n",
      "376 ('오프라인', 1)\n",
      "377 ('처', 1)\n",
      "378 ('의합', 1)\n",
      "379 ('니', 1)\n",
      "380 ('해당', 1)\n",
      "381 ('되었습니다', 1)\n",
      "382 ('침대', 1)\n",
      "383 ('연기', 1)\n",
      "384 ('해주세오', 1)\n",
      "385 ('짜리', 1)\n",
      "386 ('스케줄', 1)\n",
      "387 ('목', 1)\n",
      "388 ('로', 1)\n",
      "389 ('놔두지', 1)\n",
      "390 ('하실', 1)\n",
      "391 ('세탁기', 1)\n",
      "392 ('놓고', 1)\n",
      "393 ('가', 1)\n",
      "394 ('안오나요', 1)\n",
      "395 ('건', 1)\n",
      "396 ('한', 1)\n",
      "397 ('싶은건데', 1)\n",
      "398 ('되었네요', 1)\n",
      "399 ('하우', 1)\n",
      "400 ('온라인', 1)\n",
      "401 ('중복', 1)\n",
      "402 ('되었는데', 1)\n",
      "403 ('완료', 1)\n",
      "404 ('에러', 1)\n",
      "405 ('났어요', 1)\n",
      "406 ('카드', 1)\n",
      "407 ('되었다고', 1)\n",
      "408 ('뭐', 1)\n",
      "409 ('있어요', 1)\n",
      "410 ('끝나나요', 1)\n",
      "411 ('있어서요', 1)\n",
      "412 ('했어요', 1)\n",
      "413 ('되겠죠', 1)\n",
      "414 ('목동', 1)\n",
      "415 ('받기로', 1)\n",
      "416 ('애가', 1)\n",
      "417 ('먹던것보다', 1)\n",
      "418 ('향', 1)\n",
      "419 ('진해서', 1)\n",
      "420 ('약한', 1)\n",
      "421 ('하길', 1)\n",
      "422 ('희망', 1)\n",
      "423 ('먹여도', 1)\n",
      "424 ('성분', 1)\n",
      "425 ('과', 1)\n",
      "426 ('최근', 1)\n",
      "427 ('속성', 1)\n",
      "428 ('중요한데', 1)\n",
      "429 ('되니', 1)\n",
      "430 ('의욕', 1)\n",
      "431 ('사라지네요', 1)\n",
      "432 ('되지', 1)\n",
      "433 ('순두부', 1)\n",
      "434 ('주는데', 1)\n",
      "435 ('나왔습니다', 1)\n",
      "436 ('교환', 1)\n",
      "437 ('또한', 1)\n",
      "438 ('하려는데', 1)\n",
      "439 ('안되요', 1)\n",
      "440 ('월계동', 1)\n",
      "441 ('나옵니다', 1)\n",
      "442 ('집중', 1)\n",
      "443 ('전자렌지', 1)\n",
      "444 ('없는데', 1)\n",
      "445 ('차갑고', 1)\n",
      "446 ('맛', 1)\n",
      "447 ('떨어져도', 1)\n",
      "448 ('섭취', 1)\n",
      "449 ('되긴', 1)\n",
      "450 ('하는거죠', 1)\n",
      "451 ('라떼', 1)\n",
      "452 ('당뇨', 1)\n",
      "453 ('환자', 1)\n",
      "454 ('괜찮은', 1)\n",
      "455 ('동생', 1)\n",
      "456 ('먹이려니', 1)\n",
      "457 ('나와서요', 1)\n",
      "458 ('어찌', 1)\n",
      "459 ('하다가', 1)\n",
      "460 ('난', 1)\n",
      "461 ('예전', 1)\n",
      "462 ('주일', 1)\n",
      "463 ('미뤄서', 1)\n",
      "464 ('받았으면', 1)\n",
      "465 ('미룰수있나요', 1)\n",
      "466 ('여긴', 1)\n",
      "467 ('서울', 1)\n",
      "468 ('창', 1)\n",
      "469 ('동점', 1)\n",
      "470 ('떡볶이', 1)\n",
      "471 ('점심', 1)\n",
      "472 ('통과', 1)\n",
      "473 ('머리카락', 1)\n",
      "474 ('나왔어요', 1)\n",
      "475 ('일요일', 1)\n",
      "476 ('제주도', 1)\n",
      "477 ('여행', 1)\n",
      "478 ('일주', 1)\n",
      "479 ('일치', 1)\n",
      "480 ('부경', 1)\n",
      "481 ('김동수', 1)\n",
      "482 ('죄송하지만', 1)\n",
      "483 ('내주신다', 1)\n",
      "484 ('낼', 1)\n",
      "485 ('수욜', 1)\n",
      "486 ('토욜', 1)\n",
      "487 ('담주에', 1)\n",
      "488 ('주셔도', 1)\n",
      "489 ('감사히', 1)\n",
      "490 ('받겠습니다', 1)\n",
      "491 ('하며', 1)\n",
      "492 ('먹을거라고', 1)\n",
      "493 ('말씀드렸었는데', 1)\n",
      "494 ('된가요', 1)\n",
      "495 ('식', 1)\n",
      "496 ('받았습니다', 1)\n",
      "497 ('기대했던것과', 1)\n",
      "498 ('달라', 1)\n",
      "499 ('받던지', 1)\n",
      "500 ('받고자', 1)\n",
      "501 ('원해요', 1)\n",
      "502 ('개발', 1)\n",
      "503 ('계획', 1)\n",
      "504 ('퀄리티', 1)\n",
      "505 ('좋아', 1)\n",
      "506 ('만족스러워', 1)\n",
      "507 ('꾸준히', 1)\n",
      "508 ('먹고싶은데', 1)\n",
      "509 ('가짓수', 1)\n",
      "510 ('적어', 1)\n",
      "511 ('치니', 1)\n",
      "512 ('아쉽네요', 1)\n",
      "513 ('정해졌는데', 1)\n",
      "514 ('받고싶어요', 1)\n",
      "515 ('텀', 1)\n",
      "516 ('생겨서요', 1)\n",
      "517 ('꺼내놓기', 1)\n",
      "518 ('힘들어서', 1)\n",
      "519 ('종이가방', 1)\n",
      "520 ('가능하다해', 1)\n",
      "521 ('내', 1)\n",
      "522 ('구리시', 1)\n",
      "523 ('호로', 1)\n",
      "524 ('아파트', 1)\n",
      "525 ('비번', 1)\n",
      "526 ('없던데', 1)\n",
      "527 ('먹일려고', 1)\n",
      "528 ('딱', 1)\n",
      "529 ('준비', 1)\n",
      "530 ('식물성', 1)\n",
      "531 ('유산균', 1)\n",
      "532 ('쌀', 1)\n",
      "533 ('은색', 1)\n",
      "534 ('들어가있습니다', 1)\n",
      "535 ('색', 1)\n",
      "536 ('똑같아서', 1)\n",
      "537 ('뚜껑', 1)\n",
      "538 ('열어', 1)\n",
      "539 ('버렸습니다', 1)\n",
      "540 ('말씀드려야하나요', 1)\n",
      "541 ('됐어요', 1)\n",
      "542 ('신랑', 1)\n",
      "543 ('이름', 1)\n",
      "544 ('시켰는데', 1)\n",
      "545 ('카톡', 1)\n",
      "546 ('가능하게', 1)\n",
      "547 ('편식', 1)\n",
      "548 ('심하고', 1)\n",
      "549 ('낳고', 1)\n",
      "550 ('갖춰', 1)\n",
      "551 ('먹지', 1)\n",
      "552 ('끝났는데', 1)\n",
      "553 ('더러워요', 1)\n",
      "554 ('사용', 1)\n",
      "555 ('있을', 1)\n",
      "556 ('좋네요', 1)\n",
      "557 ('여유', 1)\n",
      "558 ('해주시고', 1)\n",
      "559 ('주셨는데', 1)\n",
      "560 ('제일', 1)\n",
      "561 ('깨끗한', 1)\n",
      "562 ('이어서', 1)\n",
      "563 ('먹었어서', 1)\n",
      "564 ('두번째', 1)\n",
      "565 ('모든', 1)\n",
      "566 ('오이', 1)\n",
      "567 ('뺄', 1)\n",
      "568 ('문', 1)\n",
      "569 ('걸어', 1)\n",
      "570 ('두면', 1)\n",
      "571 ('하던', 1)\n",
      "572 ('시키면', 1)\n",
      "573 ('주는', 1)\n",
      "574 ('되었나요', 1)\n",
      "575 ('받다가', 1)\n",
      "576 ('받았어요', 1)\n",
      "577 ('왔구요', 1)\n",
      "578 ('원래', 1)\n",
      "579 ('연달', 1)\n",
      "580 ('서', 1)\n",
      "581 ('맞는거', 1)\n",
      "582 ('다시', 1)\n",
      "583 ('되는', 1)\n",
      "584 ('품주는것도', 1)\n",
      "585 ('인해', 1)\n",
      "586 ('구체', 1)\n",
      "587 ('싶은데요', 1)\n",
      "588 ('해지', 1)\n",
      "589 ('되는걸', 1)\n",
      "590 ('볼수있을까요', 1)\n",
      "591 ('문후', 1)\n",
      "592 ('되는거', 1)\n",
      "593 ('맞나요', 1)\n",
      "594 ('두', 1)\n",
      "595 ('받을께요', 1)\n",
      "596 ('용기', 1)\n",
      "597 ('깨져있습니다', 1)\n",
      "598 ('날카롭게', 1)\n",
      "599 ('되있고', 1)\n",
      "600 ('조각', 1)\n",
      "601 ('먹게', 1)\n",
      "602 ('될', 1)\n",
      "603 ('위험', 1)\n",
      "604 ('있었습니다', 1)\n",
      "605 ('이랬다', 1)\n",
      "606 ('저랬다', 1)\n",
      "607 ('껄', 1)\n",
      "608 ('보기', 1)\n",
      "609 ('하고요', 1)\n",
      "610 ('했구요', 1)\n",
      "611 ('드린', 1)\n",
      "612 ('바', 1)\n",
      "613 ('팀', 1)\n",
      "614 ('혹은', 1)\n",
      "615 ('오늘아침', 1)\n",
      "616 ('맘대로', 1)\n",
      "617 ('바뀌어져있네요', 1)\n",
      "618 ('월화', 1)\n",
      "619 ('목금', 1)\n",
      "620 ('해놓았는데', 1)\n",
      "621 ('걸', 1)\n",
      "622 ('다시주세요', 1)\n",
      "623 ('돈', 1)\n",
      "624 ('주고', 1)\n",
      "625 ('산건데', 1)\n",
      "626 ('별로', 1)\n",
      "627 ('전달', 1)\n",
      "628 ('해주시길', 1)\n",
      "629 ('와서', 1)\n",
      "630 ('세로', 1)\n",
      "631 ('세워져있는', 1)\n",
      "632 ('바람', 1)\n",
      "633 ('샜음', 1)\n",
      "634 ('진짜', 1)\n",
      "635 ('짜증나네요', 1)\n",
      "636 ('의', 1)\n",
      "637 ('알고싶어요', 1)\n",
      "638 ('거보', 1)\n",
      "639 ('빠져', 1)\n",
      "640 ('저지방우유', 1)\n",
      "641 ('작일', 1)\n",
      "642 ('따로', 1)\n",
      "643 ('정도', 1)\n",
      "644 ('걸린다', 1)\n",
      "645 ('얘기', 1)\n",
      "646 ('적혀있었는데', 1)\n",
      "647 ('안되어서요', 1)\n",
      "648 ('치면', 1)\n",
      "649 ('보관', 1)\n",
      "650 ('어려울', 1)\n",
      "651 ('되는지', 1)\n",
      "652 ('임시', 1)\n",
      "653 ('생각', 1)\n",
      "654 ('했네요', 1)\n",
      "655 ('그렇다면', 1)\n",
      "656 ('나온', 1)\n",
      "657 ('콩나물', 1)\n",
      "658 ('밥', 1)\n",
      "659 ('심각하게', 1)\n",
      "660 ('설익었습니다', 1)\n",
      "661 ('생쌀', 1)\n",
      "662 ('먹는줄', 1)\n",
      "663 ('알았어요', 1)\n",
      "664 ('익혀주세요', 1)\n",
      "665 ('지점', 1)\n",
      "666 ('및', 1)\n",
      "667 ('안내', 1)\n",
      "668 ('고지', 1)\n",
      "669 ('않음', 1)\n",
      "670 ('서비스', 1)\n",
      "671 ('중간', 1)\n",
      "672 ('미뤄지는거죠', 1)\n",
      "673 ('유지', 1)\n",
      "674 ('이용', 1)\n",
      "675 ('표', 1)\n",
      "676 ('간식', 1)\n",
      "677 ('제공', 1)\n",
      "678 ('집앞', 1)\n",
      "679 ('야구르트', 1)\n",
      "680 ('백안', 1)\n",
      "681 ('넣어', 1)\n",
      "682 ('두시', 1)\n",
      "683 ('가지고가셨네요', 1)\n",
      "684 ('이건', 1)\n",
      "685 ('무슨', 1)\n",
      "686 ('혈행', 1)\n",
      "687 ('개선', 1)\n",
      "688 ('효과', 1)\n",
      "689 ('좋은', 1)\n",
      "690 ('맛있나요', 1)\n",
      "691 ('카페', 1)\n",
      "692 ('망고', 1)\n",
      "693 ('사과', 1)\n",
      "694 ('쥬스', 1)\n",
      "695 ('오기', 1)\n",
      "696 ('사항', 1)\n",
      "697 ('톡', 1)\n",
      "698 ('받아서요', 1)\n",
      "699 ('부탁드리겠습니다', 1)\n",
      "700 ('받았는데', 1)\n",
      "701 ('오는거죠', 1)\n",
      "702 ('평일', 1)\n",
      "703 ('안되는건', 1)\n",
      "704 ('토요일', 1)\n",
      "705 ('쯤', 1)\n",
      "706 ('오는지', 1)\n",
      "707 ('오긴', 1)\n",
      "708 ('금주', 1)\n",
      "709 ('인하여', 1)\n",
      "710 ('됐는데', 1)\n",
      "711 ('상의', 1)\n",
      "712 ('착오', 1)\n",
      "713 ('아닌가', 1)\n",
      "714 ('있는대요', 1)\n",
      "715 ('소바', 1)\n",
      "716 ('행사', 1)\n",
      "717 ('용이', 1)\n",
      "718 ('받고있고', 1)\n",
      "719 ('오전', 1)\n",
      "720 ('먹이고있는데', 1)\n",
      "721 ('먹으며', 1)\n",
      "722 ('양', 1)\n",
      "723 ('늘려야', 1)\n",
      "724 ('할지', 1)\n",
      "725 ('시기', 1)\n",
      "726 ('연휴', 1)\n",
      "727 ('욜', 1)\n",
      "728 ('보는데', 1)\n",
      "729 ('문제', 1)\n",
      "730 ('없을까요', 1)\n",
      "731 ('달이', 1)\n",
      "732 ('먹으라고', 1)\n",
      "733 ('하셨는데', 1)\n",
      "734 ('알아서', 1)\n",
      "735 ('바꿔서', 1)\n",
      "736 ('놓은', 1)\n",
      "737 ('알레르기', 1)\n",
      "738 ('첨', 1)\n",
      "739 ('이해', 1)\n",
      "740 ('잘안가서', 1)\n",
      "741 ('그러는데', 1)\n",
      "742 ('먹을', 1)\n",
      "743 ('먹으라는', 1)\n",
      "744 ('몇번', 1)\n",
      "745 ('나눠', 1)\n",
      "746 ('서라도', 1)\n",
      "747 ('먹게하라는', 1)\n",
      "748 ('몰라서요', 1)\n",
      "749 ('그래서', 1)\n",
      "750 ('먹는걸', 1)\n",
      "751 ('넘어간다는', 1)\n",
      "752 ('저녁', 1)\n",
      "753 ('치킨', 1)\n",
      "754 ('먹으려고', 1)\n",
      "755 ('유기농', 1)\n",
      "756 ('계란', 1)\n",
      "757 ('해요', 1)\n",
      "758 ('쉐이크', 1)\n",
      "759 ('품', 1)\n",
      "760 ('안오네요', 1)\n",
      "761 ('보름', 1)\n",
      "762 ('보내준다고', 1)\n",
      "763 ('했던것', 1)\n",
      "764 ('이틀', 1)\n",
      "765 ('대문', 1)\n",
      "766 ('대부분', 1)\n",
      "767 ('누락', 1)\n",
      "768 ('정지', 1)\n",
      "769 ('목요일', 1)\n",
      "770 ('한꺼', 1)\n",
      "771 ('남자', 1)\n",
      "772 ('있어서', 1)\n",
      "773 ('먹이려는데', 1)\n",
      "774 ('어린이집', 1)\n",
      "775 ('보내야', 1)\n",
      "776 ('푸딩', 1)\n",
      "777 ('살아이가', 1)\n",
      "778 ('제조', 1)\n",
      "779 ('신선하지', 1)\n",
      "780 ('토핑', 1)\n",
      "781 ('소스', 1)\n",
      "782 ('개옴', 1)\n",
      "783 ('할려고', 1)\n",
      "784 ('인천', 1)\n",
      "785 ('아님', 1)\n",
      "786 ('받는거예요', 1)\n",
      "787 ('몇개', 1)\n",
      "788 ('배', 1)\n",
      "789 ('송료', 1)\n",
      "790 ('브로콜리', 1)\n",
      "791 ('음료', 1)\n",
      "792 ('받는거', 1)\n",
      "793 ('선택', 1)\n",
      "794 ('받는거로', 1)\n",
      "795 ('하려고하는데', 1)\n",
      "796 ('더라구요', 1)\n",
      "797 ('그러면', 1)\n",
      "798 ('해주시나요', 1)\n",
      "799 ('그람', 1)\n",
      "800 ('사와서', 1)\n",
      "801 ('뜯었는데', 1)\n",
      "802 ('터져있네요', 1)\n",
      "803 ('찝찝해서', 1)\n",
      "804 ('먹지도', 1)\n",
      "805 ('하겠구요', 1)\n",
      "806 ('이런', 1)\n",
      "807 ('남깁니다', 1)\n",
      "808 ('폰', 1)\n",
      "809 ('남기기', 1)\n",
      "810 ('어렵더군요', 1)\n",
      "811 ('얼마', 1)\n",
      "812 ('청소기', 1)\n",
      "813 ('볼', 1)\n",
      "814 ('있을지', 1)\n",
      "815 ('궁금해서', 1)\n",
      "816 ('하는거', 1)\n",
      "817 ('할려면', 1)\n",
      "818 ('여기', 1)\n",
      "819 ('해도', 1)\n",
      "820 ('그날', 1)\n",
      "821 ('연장', 1)\n",
      "822 ('스케쥴', 1)\n",
      "823 ('작성', 1)\n",
      "824 ('된게', 1)\n",
      "825 ('음', 1)\n",
      "826 ('뜨네요', 1)\n",
      "827 ('날로', 1)\n",
      "828 ('했었는데', 1)\n",
      "829 ('지나면', 1)\n",
      "830 ('자동', 1)\n",
      "831 ('상세', 1)\n",
      "832 ('되는데요', 1)\n",
      "833 ('원룸', 1)\n",
      "834 ('거주', 1)\n",
      "835 ('하는데요', 1)\n",
      "836 ('건물', 1)\n",
      "837 ('두고', 1)\n",
      "838 ('방문', 1)\n",
      "839 ('옮기고', 1)\n",
      "840 ('기사', 1)\n",
      "841 ('초인종', 1)\n",
      "842 ('누르면', 1)\n",
      "843 ('여', 1)\n",
      "844 ('방법', 1)\n",
      "845 ('바꾸고', 1)\n",
      "846 ('늦은', 1)\n",
      "847 ('새벽', 1)\n",
      "848 ('상관없어요', 1)\n",
      "849 ('상자', 1)\n",
      "850 ('첫', 1)\n",
      "851 ('전날', 1)\n",
      "852 ('미리', 1)\n",
      "853 ('당일', 1)\n",
      "854 ('싶은데', 1)\n",
      "855 ('사이트', 1)\n",
      "856 ('접속', 1)\n",
      "857 ('하니', 1)\n",
      "858 ('계속', 1)\n",
      "859 ('나서요', 1)\n",
      "860 ('예정일', 1)\n",
      "861 ('뜹니다', 1)\n",
      "862 ('답답', 1)\n",
      "863 ('좋아하는', 1)\n",
      "864 ('할수', 1)\n",
      "865 ('사촌언니', 1)\n",
      "866 ('네사', 1)\n",
      "867 ('얻어', 1)\n",
      "868 ('좋아해서', 1)\n",
      "869 ('끌어안고', 1)\n",
      "870 ('타고', 1)\n",
      "871 ('나갔다가', 1)\n",
      "872 ('떨어뜨린것', 1)\n",
      "873 ('유유', 1)\n",
      "874 ('평균', 1)\n",
      "875 ('나트륨', 1)\n",
      "876 ('네', 1)\n",
      "877 ('감사해요', 1)\n",
      "878 ('기다릴테니', 1)\n",
      "879 ('관련', 1)\n",
      "880 ('부서', 1)\n",
      "881 ('하셔서', 1)\n",
      "882 ('부탁드릴게요', 1)\n",
      "883 ('차라리', 1)\n",
      "884 ('파는거면', 1)\n",
      "885 ('당장', 1)\n",
      "886 ('사겠는데', 1)\n",
      "887 ('그것', 1)\n",
      "888 ('아니라서', 1)\n",
      "889 ('하기가', 1)\n",
      "890 ('어렵네요', 1)\n",
      "891 ('파란', 1)\n",
      "892 ('안고다니구', 1)\n",
      "893 ('탈때', 1)\n",
      "894 ('쥐어주니', 1)\n",
      "895 ('타길래', 1)\n",
      "896 ('들고', 1)\n",
      "897 ('나갔다', 1)\n",
      "898 ('그만', 1)\n",
      "899 ('그래도', 1)\n",
      "900 ('통해', 1)\n",
      "901 ('먹고있는데', 1)\n",
      "902 ('애용', 1)\n",
      "903 ('할테니', 1)\n",
      "904 ('알아봐주세요', 1)\n",
      "905 ('하게되', 1)\n",
      "906 ('면', 1)\n",
      "907 ('맨날', 1)\n",
      "908 ('노래', 1)\n",
      "909 ('부르고', 1)\n",
      "910 ('다닐게요', 1)\n",
      "911 ('정말', 1)\n",
      "912 ('감사합니다', 1)\n",
      "913 ('박스', 1)\n",
      "914 ('중지', 1)\n",
      "915 ('오질', 1)\n",
      "916 ('되는게', 1)\n",
      "917 ('금요일', 1)\n",
      "918 ('있거든요', 1)\n",
      "919 ('안된다면', 1)\n",
      "920 ('신규', 1)\n",
      "921 ('지인', 1)\n",
      "922 ('아이디', 1)\n",
      "923 ('없으면', 1)\n",
      "924 ('할인', 1)\n",
      "925 ('사제품', 1)\n",
      "926 ('프로', 1)\n",
      "927 ('해주던데', 1)\n",
      "928 ('이었습니다', 1)\n",
      "929 ('포장', 1)\n",
      "930 ('뜯고', 1)\n",
      "931 ('찍은', 1)\n",
      "932 ('사진', 1)\n",
      "933 ('가끔', 1)\n",
      "934 ('성의', 1)\n",
      "935 ('담아', 1)\n",
      "936 ('보낼', 1)\n",
      "937 ('있더군요', 1)\n",
      "938 ('먹다', 1)\n",
      "939 ('싸준거', 1)\n",
      "940 ('같아서', 1)\n",
      "941 ('좋지', 1)\n",
      "942 ('제작', 1)\n",
      "943 ('과정', 1)\n",
      "944 ('그럴수', 1)\n",
      "945 ('있겠지만', 1)\n",
      "946 ('신경', 1)\n",
      "947 ('쓰셔야', 1)\n",
      "948 ('할거', 1)\n",
      "949 ('거부', 1)\n",
      "950 ('바랍니다', 1)\n",
      "951 ('잘못', 1)\n",
      "952 ('봐두', 1)\n",
      "953 ('최종', 1)\n",
      "954 ('까지인가요', 1)\n",
      "955 ('정확', 1)\n",
      "956 ('국내', 1)\n",
      "957 ('산', 1)\n",
      "958 ('쓰나요', 1)\n",
      "959 ('치즈', 1)\n",
      "960 ('어떤', 1)\n",
      "961 ('되네요', 1)\n",
      "962 ('이', 1)\n",
      "963 ('포함', 1)\n",
      "964 ('말아주세요', 1)\n",
      "965 ('요즘', 1)\n",
      "966 ('자주', 1)\n",
      "967 ('내용물', 1)\n",
      "968 ('넘쳐있어요', 1)\n",
      "969 ('짜증', 1)\n",
      "970 ('이나', 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "import numpy as np\n",
    "\n",
    "wordId_dic = {}\n",
    "idx = 0\n",
    "for key in keys:\n",
    "    idx += 1\n",
    "    print(idx , key)\n",
    "    wordId_dic[key[0]] = idx  # wordId_dic 글자들과 순서를 갖는 dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([73, 788, 164, 962, 344, 395, 876, 11]),\n",
       "       list([962, 17, 115, 361, 36, 36, 962, 314, 788, 962, 75, 906, 788, 164, 962, 277, 388, 876, 11, 521, 95, 36, 256, 962, 194, 74, 788, 164, 115, 8, 11]),\n",
       "       list([788, 962, 55, 379, 71, 17, 36, 23, 69, 8, 11, 36, 379, 71]),\n",
       "       list([115, 87, 379, 17, 393, 962, 738, 361, 11, 962, 75, 788, 388, 77, 361, 8, 145, 962, 361, 194, 17, 8, 81, 363, 363, 962, 580, 11, 808, 49, 153, 393, 8, 81, 45, 24, 11, 8, 331, 45, 111, 521, 8, 36, 395, 393, 115, 580, 11]),\n",
       "       list([208, 95, 594, 962, 361, 331, 395, 393, 11]),\n",
       "       list([47, 788, 11]),\n",
       "       list([788, 217, 962, 379, 361, 788, 388, 111, 521, 8, 36, 906, 55, 11]),\n",
       "       list([388, 788, 164, 962, 11]),\n",
       "       list([213, 100, 788, 393, 217, 962, 379, 361, 788, 788, 55, 11]),\n",
       "       list([962, 36, 256, 396, 788, 788, 393, 396, 11]),\n",
       "       list([788, 145, 115, 8, 11]),\n",
       "       list([11, 75, 188, 377, 568, 636, 379, 71]),\n",
       "       list([115, 95, 759, 78, 788, 379, 71]),\n",
       "       list([788, 8, 568, 379, 71, 47, 95, 788, 115, 8]),\n",
       "       list([8, 18, 188, 361, 788, 8, 153, 379, 71]),\n",
       "       list([8, 568, 8, 81, 47, 208, 8, 580, 209, 387, 8, 388, 379, 71, 393, 83, 11]),\n",
       "       list([393, 568, 80, 594, 11]),\n",
       "       list([521, 95, 788, 48, 211, 393, 8, 11]),\n",
       "       list([213, 81, 8, 568, 361, 17, 55, 11]), list([788, 361, 55, 11]),\n",
       "       list([213, 8, 18, 188, 396, 395, 101, 8, 396, 18, 395, 361, 11, 208, 8, 18, 331, 876, 11, 8, 55, 396, 18, 331, 115, 8, 11]),\n",
       "       list([393, 388, 759, 8, 568, 83, 48, 962, 78, 17, 396, 393, 216, 393, 388, 17, 115, 11]),\n",
       "       list([788, 164, 36, 17, 393, 11]),\n",
       "       list([8, 568, 962, 145, 17, 393, 361, 8, 568, 64, 395, 393, 11, 145, 393, 11]),\n",
       "       list([17, 393, 71, 568, 393, 361, 73, 962, 580, 8, 568, 521, 217, 962, 11]),\n",
       "       list([393, 408, 393, 11]), list([55, 11]),\n",
       "       list([95, 906, 17, 788, 962, 11, 95, 962, 580, 11]),\n",
       "       list([788, 906, 331, 115, 11]),\n",
       "       list([208, 580, 71, 962, 759, 8, 8, 568, 11, 47, 95, 788]),\n",
       "       list([55, 11, 387, 361, 11, 95, 788, 388, 361, 11, 49, 379, 71]),\n",
       "       list([81, 8, 568, 115, 580, 49, 361, 213, 393, 77, 111, 71, 418, 962, 115, 580, 23, 87, 396, 77, 388, 8, 568, 379, 71, 521, 95, 393, 83, 11]),\n",
       "       list([788]), list([256, 81, 843, 11]),\n",
       "       list([153, 18, 47, 962, 81, 11]),\n",
       "       list([71, 962, 388, 277, 188, 95, 788, 361, 256, 111, 379, 962, 425, 71, 64, 495, 759, 962, 379, 71, 71, 759, 388, 393, 396, 393, 11]),\n",
       "       list([788, 568, 636]),\n",
       "       list([788, 145, 78, 788, 71, 962, 495, 759, 962, 145, 11, 396, 361, 36, 594, 49, 962, 78, 788, 379, 83, 636, 962, 876, 11]),\n",
       "       list([788, 36, 962, 331, 11, 594, 759, 962, 788, 379, 71]),\n",
       "       list([594, 81, 580, 18, 47, 396, 8, 361, 962, 962, 379, 71, 115, 8, 11]),\n",
       "       list([216, 396, 256, 81, 361, 55, 11]),\n",
       "       list([47, 361, 69, 276, 64, 962, 379, 71]),\n",
       "       list([71, 962, 100, 145, 8, 850, 188, 379, 71, 194, 393, 361, 277, 11]),\n",
       "       list([393, 446, 962, 906, 277, 75]),\n",
       "       list([962, 393, 395, 393, 11]),\n",
       "       list([379, 393, 788, 164, 115, 580, 361, 331, 788, 164, 115, 580, 962, 379, 393, 188, 115, 396, 71, 580, 11, 962, 78, 393, 361, 115, 11]),\n",
       "       list([361, 17, 395, 393, 11]), list([393, 11, 71, 393, 11]),\n",
       "       list([759, 788, 66, 71, 393, 393, 460, 109, 388, 24, 962, 55, 379, 71, 194, 66, 580, 8, 95, 78, 580, 47, 95, 788, 906, 379, 71]),\n",
       "       list([8, 568, 245, 962, 580, 95, 788, 36, 36, 17, 705, 568, 636, 11]),\n",
       "       list([95, 962, 788, 66, 361, 396, 393, 788, 395, 393, 11, 594, 393, 788, 395, 393, 11]),\n",
       "       list([8, 388, 277, 8, 568, 115, 580, 521, 95, 36, 361, 11, 71, 825, 8, 47, 208, 788, 71, 825, 8, 388, 78, 24, 11]),\n",
       "       list([843, 580, 361, 11, 962, 468, 580, 249, 962, 249, 580, 388, 361, 249, 425, 396, 962, 11]),\n",
       "       list([962, 49, 8, 95, 11, 95, 17, 8, 843, 48, 568, 36, 8, 568, 361, 95, 8, 95, 396, 49, 788, 164, 393, 396, 393, 11]),\n",
       "       list([568, 636, 24, 379, 71, 153, 71, 17, 759, 388, 111, 521, 8, 71, 361, 36, 484, 24, 11, 962, 49, 8, 24, 727, 727, 100, 759, 962, 580, 11, 379, 906, 8, 8, 379, 71]),\n",
       "       list([36, 18, 8, 568, 361, 18, 153, 788, 379, 71, 331, 64, 395, 393, 11]),\n",
       "       list([521, 95, 36, 788, 164, 11, 36, 75, 361, 11]),\n",
       "       list([95, 36, 788, 11]),\n",
       "       list([100, 393, 361, 621, 906, 64, 393, 11]),\n",
       "       list([95, 495, 8, 256, 8, 568, 115, 580, 377, 825, 788, 379, 71, 17, 393, 77, 425, 164, 580, 55, 379, 906, 71, 759, 388, 115, 580, 361, 393, 396, 393, 11]),\n",
       "       list([8, 568, 115, 11]),\n",
       "       list([36, 18, 962, 11, 788, 164, 36, 145, 393, 153, 361, 393, 24, 393, 363, 259, 379, 876, 11, 23, 87, 393, 115, 8, 11]),\n",
       "       list([36, 788, 8, 11, 379, 71]),\n",
       "       list([188, 379, 71, 95, 788, 962, 115, 361, 36, 95, 788, 24, 906, 95, 11, 515, 962, 23, 580, 11]),\n",
       "       list([8, 11, 379, 71]),\n",
       "       list([788, 393, 580, 393, 521, 580, 95, 962, 393, 388, 788, 393, 83, 11, 8, 568, 245, 962, 580, 95, 788, 36, 36, 17, 705, 568, 636, 11]),\n",
       "       list([71, 825, 164, 8, 568, 906, 580, 115, 11]),\n",
       "       list([256, 393, 71, 115, 379, 71, 256, 379, 71]),\n",
       "       list([393, 396, 36, 95, 521, 188, 36, 388, 788, 379, 71]),\n",
       "       list([49, 379, 71]), list([256, 256, 379, 71]),\n",
       "       list([18, 47, 256, 81, 377, 825, 36, 379, 71, 962, 256, 361, 8, 568, 962, 393, 396, 393, 11]),\n",
       "       list([17, 11]), list([17, 11, 95, 528, 361]),\n",
       "       list([495, 957, 532, 17, 759, 393, 11]),\n",
       "       list([55, 535, 962, 962, 393, 379, 71]), list([568, 636, 379, 71]),\n",
       "       list([788, 164, 962, 69, 276, 361, 535, 962, 580, 379, 71, 11]),\n",
       "       list([256, 788, 164, 361, 256, 153, 788, 164, 11]),\n",
       "       list([788, 95, 379, 71]),\n",
       "       list([962, 388, 393, 115, 580, 788, 36, 361, 17, 697, 388, 8, 568, 393, 331, 115, 8, 11]),\n",
       "       list([277, 55, 395, 495, 759, 759, 8, 568, 843, 69, 379, 71, 495, 17, 388, 276, 361, 331, 69, 379, 71, 962, 17, 788, 164, 962, 361, 45, 273, 393, 17, 393, 188, 396, 395, 361, 393, 87, 11, 100, 580, 331, 83, 24, 75, 361, 256, 962, 55, 876, 11, 277, 55, 393, 276, 621, 906, 843, 256, 788, 164, 115, 8, 36, 69, 115, 8, 361, 277, 145, 580, 17, 95, 55, 396, 75, 11]),\n",
       "       list([278, 393, 388, 962, 580, 8, 11]),\n",
       "       list([393, 11, 379, 71, 379, 393, 594, 788, 164, 115, 580, 393, 962, 379, 71]),\n",
       "       list([594, 49, 78, 788, 379, 71]),\n",
       "       list([95, 788, 256, 962, 55, 11, 71, 825, 8, 47, 11, 95, 788, 379, 71]),\n",
       "       list([71, 962, 495, 759, 580, 962, 81, 567, 24, 11]),\n",
       "       list([788, 962, 17, 602, 11, 379, 71]),\n",
       "       list([64, 393, 568, 80, 621, 594, 906, 11]),\n",
       "       list([47, 256, 8, 81, 36, 906, 8, 962, 11]),\n",
       "       list([17, 393, 95, 788, 71, 393, 95, 495, 276, 11, 95, 216, 188, 11, 788, 962, 95, 379, 55, 75, 164, 580, 111, 521, 8, 36, 906, 95, 788, 962, 75, 361, 11, 788, 71, 36, 396, 49, 115, 8, 11]),\n",
       "       list([277, 208, 962, 11, 759, 8, 77, 71, 393, 11]),\n",
       "       list([55, 11, 71, 962, 495, 759, 8, 115, 580, 788, 361, 11, 393, 388, 8, 256, 87, 8, 568, 361, 47, 850, 8, 393, 388, 115, 55, 115, 580, 788, 66, 81, 23, 379, 71, 188, 363, 388, 47, 95, 788, 36, 115, 580, 47, 850, 8, 395, 47, 95, 580, 47, 95, 788, 361, 11, 393, 396, 393, 11]),\n",
       "       list([115, 379, 71]),\n",
       "       list([95, 788, 164, 621, 95, 388, 813, 24, 11, 17, 393, 95, 962, 95, 962, 580, 11]),\n",
       "       list([962, 78, 188, 396, 393, 962, 906, 580, 188, 36, 393, 393, 188, 55, 115, 395, 393, 11]),\n",
       "       list([17, 759, 18, 81, 8, 568, 314, 17, 759, 66, 962, 48, 393, 906, 75, 11]),\n",
       "       list([277, 277, 95, 396, 81, 111, 621, 388, 83, 11, 47, 95, 95, 396, 594, 11, 47, 95, 580, 11]),\n",
       "       list([393, 379, 71, 66, 331, 36, 331, 602, 211, 379, 71]),\n",
       "       list([962, 71, 213, 71, 115, 11, 277, 209, 11, 95, 607, 47, 11, 95, 66, 111, 388, 361, 11, 95, 71, 825, 594, 49, 111, 621, 388, 83, 11, 115, 11]),\n",
       "       list([580, 71, 962, 495, 759, 8, 568, 361, 11, 213, 395, 393, 11]),\n",
       "       list([594, 81, 8, 568, 188, 11, 568, 636, 612, 962, 47, 95, 66, 788, 962, 314, 47, 850, 8, 395, 47, 95, 47, 95, 788, 379, 71, 115, 8, 11]),\n",
       "       list([636, 613, 256, 636, 962, 95, 8, 81, 45, 24, 11]),\n",
       "       list([393, 962, 95, 393, 388, 388, 612, 876, 11, 331, 64, 395, 393, 11]),\n",
       "       list([788, 47, 209, 24, 387, 208, 388, 115, 361, 962, 621, 47, 24, 208, 388, 11, 393, 396, 393, 11]),\n",
       "       list([521, 95, 962, 788, 164, 95, 361, 393, 388, 71, 36, 8, 11, 623, 8, 957, 395, 361, 256, 962, 388, 876, 11, 74, 194, 164, 115, 8, 36, 95, 388, 580, 388, 388, 612, 825, 495, 962, 388, 71, 825, 876, 11]),\n",
       "       list([8, 568, 396, 636, 788, 45, 11]),\n",
       "       list([788, 164, 64, 75, 111, 379, 962, 876, 11]),\n",
       "       list([95, 71, 962, 8, 388, 277, 8, 568, 361, 36, 213, 8, 388, 277, 36, 95, 962, 788, 962, 395, 393, 11, 388, 621, 71, 962, 788, 788, 962, 363, 361, 788, 962, 55, 580, 11, 495, 388, 277, 425, 259, 906, 111, 962, 77, 361, 17, 788, 45, 24, 11]),\n",
       "       list([47, 95, 962, 36, 95, 77, 276, 876, 11, 277, 71, 906, 47, 95, 788, 115, 580, 47, 788, 47, 95, 388, 788, 379, 71]),\n",
       "       list([658, 962, 331, 379, 71, 532, 45, 11, 69, 8, 11]),\n",
       "       list([153, 666, 17, 388, 55, 521, 825, 580, 153]),\n",
       "       list([71, 962, 495, 759, 580, 47, 95, 36, 47, 95, 188, 379, 71, 145, 95, 962, 47, 95, 95, 95, 361, 277, 95, 962, 109, 388, 87, 78, 75]),\n",
       "       list([788]), list([64, 788, 388, 361, 331, 906, 602, 11]),\n",
       "       list([388, 277, 962, 36, 495, 675, 81, 111, 379, 495, 962, 962, 17, 395, 393, 11]),\n",
       "       list([962, 49, 164, 153, 788, 164, 379, 71]), list([8]),\n",
       "       list([100, 80, 188, 55, 36, 594, 36, 17, 393, 188, 393, 393, 393, 876, 11, 962, 395, 393, 11]),\n",
       "       list([395, 495, 759, 145, 18, 425, 17, 759, 11]),\n",
       "       list([153, 18, 47, 962, 81, 11]), list([11, 75, 446, 11]),\n",
       "       list([11]), list([425, 612, 722, 788, 393, 379, 71]),\n",
       "       list([388, 361, 55, 11]),\n",
       "       list([256, 636, 962, 95, 8, 81, 45, 24, 11]),\n",
       "       list([256, 636, 962, 95, 8, 81, 45, 24, 11, 962, 194, 568, 636, 396, 361, 45, 697, 276, 580, 11, 379, 71]),\n",
       "       list([153, 18, 47, 962, 594, 81, 11]),\n",
       "       list([256, 115, 580, 788, 396, 71, 568, 361, 73, 256, 788, 164, 962, 55, 75]),\n",
       "       list([788, 66, 393, 331, 11]),\n",
       "       list([788, 164, 95, 55, 395, 393, 11, 11, 95, 55, 11]),\n",
       "       list([788, 962, 36, 705]), list([8, 568, 115, 8, 11]),\n",
       "       list([11, 55, 379, 71]),\n",
       "       list([208, 8, 24, 11, 95, 788, 379, 71, 24, 11, 95, 393, 388, 843, 47, 95, 788, 47, 95, 388, 379, 71]),\n",
       "       list([55, 11, 788, 164, 962, 18, 393, 361, 36, 788, 164, 636, 393, 393, 115, 580, 568, 636, 379, 71, 277, 47, 164, 95, 788, 164, 331, 11]),\n",
       "       list([36, 788, 11]), list([309, 388]),\n",
       "       list([594, 81, 95, 788, 11, 95, 788, 388, 11]),\n",
       "       list([612, 521, 962]), list([213, 636, 396, 962, 331, 11]),\n",
       "       list([47, 24, 208, 95, 388, 194, 396, 49, 153, 962, 361, 17, 594, 49, 722, 83, 36, 23, 115, 8, 11]),\n",
       "       list([962, 49, 48, 568, 47, 95, 47, 24, 208, 95, 727, 81, 71, 111, 361, 396, 568, 17, 11]),\n",
       "       list([396, 164, 55, 396, 49, 396, 164, 962, 460, 314, 594, 49, 361, 45, 580, 71, 962, 495, 759, 580, 71, 825, 388, 612, 580, 788, 164, 962, 11, 17, 47, 95, 788, 164, 621, 17, 115, 580, 11]),\n",
       "       list([45, 11]),\n",
       "       list([17, 393, 738, 962, 962, 115, 393, 69, 55, 393, 580, 277, 361, 208, 95, 388, 115, 580, 396, 18, 81, 594, 49, 580, 8, 109, 396, 49, 48, 277, 396, 18, 81, 71, 395, 396, 18, 81, 49, 580, 396, 18, 81, 71, 331, 395, 69, 580, 11, 277, 580, 396, 18, 71, 621, 396, 164, 55, 71, 825, 388, 71, 393, 11]),\n",
       "       list([213, 344, 36, 115, 111, 379, 36, 962, 379, 71]),\n",
       "       list([568, 636, 115, 11]), list([145, 906]),\n",
       "       list([612, 962, 81, 95, 8, 95, 18, 18, 47, 361, 759, 962, 55, 876, 11, 111, 55, 388, 111, 521, 71, 77, 361, 11]),\n",
       "       list([213, 962, 66, 361, 568, 80, 379, 71, 36, 256, 277, 962, 64, 395, 115, 8, 11, 36, 256, 962, 314, 396, 71, 906, 8, 568, 379, 71]),\n",
       "       list([209, 387, 11, 95, 580, 788, 164, 145, 361, 11, 47, 11, 95, 396, 49, 788, 164, 393, 83, 11]),\n",
       "       list([8, 568, 906, 393, 66, 962, 17, 788, 331, 11]),\n",
       "       list([322, 962, 361, 45, 393, 580, 594, 388, 115, 580, 962, 361, 962, 100, 111, 521, 115, 580, 36, 962, 11]),\n",
       "       list([36, 962, 77, 11, 45, 322, 962, 322, 962, 393, 11]),\n",
       "       list([47, 95, 36, 17, 17, 759, 36, 361, 850, 788, 962, 331, 788, 66, 48, 568, 153, 594, 18]),\n",
       "       list([36, 18, 8, 568, 83, 361, 8, 568, 906, 17, 11, 217, 379, 71]),\n",
       "       list([95, 788, 396, 71, 75, 11, 75, 11]),\n",
       "       list([101, 18, 393, 395, 393, 11, 309, 277, 101, 18, 393, 11]),\n",
       "       list([81, 8, 568, 906, 788, 361, 73, 101, 18, 393, 395, 393, 11]),\n",
       "       list([722, 788, 388, 825, 95, 388, 83, 188, 11]),\n",
       "       list([47, 47, 24, 208, 788, 75, 8, 95, 788, 361, 11, 47, 95, 788, 75, 388, 8, 568, 361, 87, 188, 11, 277, 906, 95, 209, 11, 95, 66, 788, 115, 8, 36, 11, 277, 36, 277, 393, 11]),\n",
       "       list([78, 788]), list([17, 705, 11]),\n",
       "       list([208, 36, 580, 361, 393, 876, 11, 115, 580, 276, 188, 11, 962, 377, 825, 962, 328, 379, 71, 962, 808, 388, 328, 87, 11]),\n",
       "       list([594, 393, 11]),\n",
       "       list([81, 208, 379, 71, 393, 66, 393, 47, 95, 962, 24, 962, 361, 11, 36, 521, 95, 962, 813, 24, 208, 115, 580, 568, 636, 379, 71]),\n",
       "       list([47, 95, 495, 788, 164, 55, 75, 83, 906, 843, 388, 115, 11, 277, 66, 788, 164, 55, 906, 109, 388, 11]),\n",
       "       list([73, 64, 331, 71, 395, 393, 11]),\n",
       "       list([36, 825, 314, 8, 361, 8, 393, 71, 876, 11, 331, 906, 11]),\n",
       "       list([47, 95, 788, 164, 71, 66, 388, 379, 71]),\n",
       "       list([377, 825, 393, 83, 48, 81, 18, 47, 361, 277, 962, 49, 164, 906, 363, 388, 788, 164, 962, 55, 395, 393, 11]),\n",
       "       list([788, 361, 8, 568, 521, 217, 962, 194, 788, 388, 361, 11]),\n",
       "       list([17, 393, 75, 8, 361, 11, 36, 788, 49, 568, 17, 48, 568, 395, 788, 393, 594, 361, 11, 788, 393, 568, 80, 788, 36, 962, 906, 17, 393, 843, 388, 612, 11, 36, 962, 962, 11]),\n",
       "       list([580, 594, 81, 8, 568, 379, 71, 850, 788, 95, 81, 379, 71]),\n",
       "       list([95, 788, 393, 11, 194, 788, 393, 11]),\n",
       "       list([47, 11, 95, 209, 11, 95, 194, 66, 78, 17, 55, 95, 876, 11, 788, 164, 36, 23, 45, 8, 11]),\n",
       "       list([788, 8, 8, 361, 36, 906, 95, 962, 55, 379, 71, 788, 95, 388, 24, 361, 962, 379, 393, 580, 11, 612, 388, 66, 962, 393, 83, 11]),\n",
       "       list([47, 788, 379, 71]),\n",
       "       list([47, 95, 788, 759, 47, 95, 788, 388, 788, 95, 115, 8, 11]),\n",
       "       list([95, 36, 361, 393, 396, 788, 95, 962, 71, 379, 71]),\n",
       "       list([47, 95, 95, 388, 115, 8, 11]),\n",
       "       list([962, 580, 188, 83, 24, 11, 393, 8, 379, 876, 361, 115, 580, 74, 55, 71, 393, 77, 11]),\n",
       "       list([594, 81, 213, 388, 393, 396, 393, 11]),\n",
       "       list([825, 495, 636]),\n",
       "       list([876, 115, 11, 71, 379, 580, 568, 636, 580, 74, 23, 331, 11, 75, 906, 361, 277, 77, 379, 580, 188, 393, 876, 11, 393, 8, 153, 55, 71, 379, 188, 48, 8, 379, 69, 71, 277, 153, 55, 277, 213, 115, 580, 8, 568, 115, 580, 594, 81, 69, 361, 80, 388, 278, 83, 379, 74, 23, 45, 8, 11, 188, 331, 906, 66, 71, 331, 11, 379, 71]),\n",
       "       list([594, 396, 788, 11]), list([145, 361, 11]),\n",
       "       list([594, 594, 81, 95, 8, 95, 17, 759, 188, 17, 17, 759, 361, 11, 17, 759, 962, 11, 95, 788, 331, 393, 11, 17, 393, 45, 273, 47, 11, 95, 580, 208, 11, 95, 621, 388, 45, 75, 11]),\n",
       "       list([71, 962, 495, 11, 55, 64, 71, 906, 361, 962, 906, 83, 55, 11, 17, 759, 388, 377, 825, 36, 83, 48, 115, 8, 361, 45, 8, 11]),\n",
       "       list([194, 396, 36, 962, 379, 71, 612, 388, 361, 11, 393, 962, 331, 636, 962, 111, 484, 48, 393, 87, 11, 71, 75, 580, 256, 962, 11, 36, 17, 425, 580, 277, 24, 153, 23, 87, 83, 75, 11]),\n",
       "       list([962, 393, 81, 75, 843, 8, 568, 612, 379, 71]),\n",
       "       list([788, 164, 962, 69, 276, 75, 361, 11]),\n",
       "       list([95, 66, 95, 962, 594, 602, 11]),\n",
       "       list([788, 95, 568, 636, 379, 71, 45, 568, 81, 361, 11, 277, 788, 164, 95, 47, 95, 393, 11, 95, 11]),\n",
       "       list([594, 521, 957, 594, 153, 11]), list([331, 11]),\n",
       "       list([47, 95, 36, 95, 388, 361, 95, 388, 95, 95, 256, 962, 876, 11, 211, 8, 568, 395, 95, 36, 95, 388, 379, 71]),\n",
       "       list([962, 8, 568, 395, 843, 95, 788, 8, 11]),\n",
       "       list([256, 81, 361, 11, 8, 521, 962, 11, 396, 75, 393, 11, 23, 962, 876, 11])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sen = []\n",
    "\n",
    "#모든 문장을 index로 바꾼다. \n",
    "for line in lines:\n",
    "    sen_idx = [wordId_dic[text] for text in line if text in word_dic]\n",
    "    all_sen.append(sen_idx)\n",
    "all_sen_arr = np.array(all_sen)\n",
    "\n",
    "X_train_all = all_sen_arr\n",
    "X_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미배송\\n', '배송일정\\n', '미배송\\n', '배송장소\\n', '미배송\\n', '배송일정\\n', '배송방법\\n', '주문\\n', '배송방법\\n', '배송방법\\n', '배송중단\\n', '구매문의\\n', '미배송\\n', '배송연기\\n', '배송오류\\n', '배송일정\\n', '가방\\n', '배송장소\\n', '미배송\\n', '미배송\\n', '배송변경\\n', '가방\\n', '배송일정\\n', '결제오류\\n', '결제오류\\n', '상품문의\\n', '인사\\n', '배송일정\\n', '배송신청\\n', '배송일정\\n', '배송장소\\n', '상품변경\\n', '배송장소\\n', '상품문의\\n', '상품문의\\n', '상품변경\\n', '배송문의\\n', '미배송\\n', '미배송\\n', '교환환불\\n', '이벤트\\n', '시스템\\n', '상품문의\\n', '상품문의\\n', '상품문의\\n', '가방\\n', '이벤트\\n', '가방\\n', '배송일정\\n', '배송일정\\n', '배송상품\\n', '배송일정\\n', '상품불량\\n', '배송문의\\n', '배송일정\\n', '배송오류\\n', '배송일정\\n', '배송일정\\n', '가방\\n', '교환환불\\n', '주문변경\\n', '상품문의\\n', '배송장소\\n', '배송일정\\n', '배송장소\\n', '배송방법\\n', '취소해지\\n', '취소해지\\n', '배송장소\\n', '배송장소\\n', '교환환불\\n', '상품문의\\n', '배송일정\\n', '배송일정\\n', '상품문의\\n', '상품불량\\n', '상품문의\\n', '배송오류\\n', '배송오류\\n', '배송일정\\n', '주문변경\\n', '가방\\n', '주문\\n', '가방\\n', '미배송\\n', '미배송\\n', '상품문의\\n', '배송일정\\n', '가방\\n', '이벤트\\n', '배송오류\\n', '이벤트\\n', '배송일정\\n', '취소해지\\n', '배송일정\\n', '가방\\n', '가방\\n', '배송상품\\n', '상품불량\\n', '배송일정\\n', '배송상품\\n', '배송일정\\n', '담당자\\n', '가방\\n', '배송일정\\n', '가방\\n', '배송일정\\n', '배송오류\\n', '미배송\\n', '배송일정\\n', '상품불량\\n', '서비스\\n', '배송일정\\n', '취소해지\\n', '배송장소\\n', '상품문의\\n', '취소해지\\n', '배송장소\\n', '가방\\n', '상품문의\\n', '상품문의\\n', '상품문의\\n', '취소해지\\n', '취소해지\\n', '이벤트\\n', '담당자\\n', '담당자\\n', '상품문의\\n', '이벤트\\n', '배송일정\\n', '배송일정\\n', '배송일정\\n', '취소해지\\n', '미배송\\n', '배송일정\\n', '배송오류\\n', '배송일정\\n', '배송변경\\n', '배송변경\\n', '이벤트\\n', '배송일정\\n', '주문\\n', '상품문의\\n', '배송문의\\n', '상품문의\\n', '상품문의\\n', '미배송\\n', '상품문의\\n', '배송일정\\n', '이벤트\\n', '미배송\\n', '배송일정\\n', '배송일정\\n', '구매문의\\n', '구매문의\\n', '배송오류\\n', '배송일정\\n', '배송일정\\n', '배송문의\\n', '배송오류\\n', '배송변경\\n', '배송일정\\n', '미배송\\n', '취소해지\\n', '상품불량\\n', '상품문의\\n', '배송일정\\n', '배송변경\\n', '배송오류\\n', '이벤트\\n', '배송변경\\n', '배송일정\\n', '배송장소\\n', '배송장소\\n', '배송일정\\n', '배송일정\\n', '배송오류\\n', '배송일정\\n', '배송문의\\n', '배송일정\\n', '배송일정\\n', '배송일정\\n', '상품문의\\n', '배송변경\\n', '상품문의\\n', '상품문의\\n', '배송문의\\n', '취소해지\\n', '미배송\\n', '이벤트\\n', '배송상품\\n', '취소해지\\n', '배송오류\\n', '배송일정\\n', '배송일정\\n', '상품문의\\n', '상품문의\\n', '배송일정\\n', '취소해지\\n', '상품불량']\n"
     ]
    }
   ],
   "source": [
    "fp2 = codecs.open('yang_class.txt', 'r')\n",
    "classes = fp2.readlines()\n",
    "cls26 = []      # y의 글자들의 리스트 (중복있음)\n",
    "class_dic = {}    # y글자와 인덱스를 갖는 dict (중복없음)\n",
    "idx = 0\n",
    "print(classes)\n",
    "for cls in classes:\n",
    "    cls = cls.replace('\\n', '')\n",
    "    cls26.append(cls)\n",
    "    \n",
    "    if not (cls in class_dic):\n",
    "        class_dic[cls] = idx\n",
    "        idx += 1\n",
    "        \n",
    "fp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미배송', '배송일정', '미배송', '배송장소', '미배송', '배송일정', '배송방법', '주문', '배송방법', '배송방법', '배송중단', '구매문의', '미배송', '배송연기', '배송오류', '배송일정', '가방', '배송장소', '미배송', '미배송', '배송변경', '가방', '배송일정', '결제오류', '결제오류', '상품문의', '인사', '배송일정', '배송신청', '배송일정', '배송장소', '상품변경', '배송장소', '상품문의', '상품문의', '상품변경', '배송문의', '미배송', '미배송', '교환환불', '이벤트', '시스템', '상품문의', '상품문의', '상품문의', '가방', '이벤트', '가방', '배송일정', '배송일정', '배송상품', '배송일정', '상품불량', '배송문의', '배송일정', '배송오류', '배송일정', '배송일정', '가방', '교환환불', '주문변경', '상품문의', '배송장소', '배송일정', '배송장소', '배송방법', '취소해지', '취소해지', '배송장소', '배송장소', '교환환불', '상품문의', '배송일정', '배송일정', '상품문의', '상품불량', '상품문의', '배송오류', '배송오류', '배송일정', '주문변경', '가방', '주문', '가방', '미배송', '미배송', '상품문의', '배송일정', '가방', '이벤트', '배송오류', '이벤트', '배송일정', '취소해지', '배송일정', '가방', '가방', '배송상품', '상품불량', '배송일정', '배송상품', '배송일정', '담당자', '가방', '배송일정', '가방', '배송일정', '배송오류', '미배송', '배송일정', '상품불량', '서비스', '배송일정', '취소해지', '배송장소', '상품문의', '취소해지', '배송장소', '가방', '상품문의', '상품문의', '상품문의', '취소해지', '취소해지', '이벤트', '담당자', '담당자', '상품문의', '이벤트', '배송일정', '배송일정', '배송일정', '취소해지', '미배송', '배송일정', '배송오류', '배송일정', '배송변경', '배송변경', '이벤트', '배송일정', '주문', '상품문의', '배송문의', '상품문의', '상품문의', '미배송', '상품문의', '배송일정', '이벤트', '미배송', '배송일정', '배송일정', '구매문의', '구매문의', '배송오류', '배송일정', '배송일정', '배송문의', '배송오류', '배송변경', '배송일정', '미배송', '취소해지', '상품불량', '상품문의', '배송일정', '배송변경', '배송오류', '이벤트', '배송변경', '배송일정', '배송장소', '배송장소', '배송일정', '배송일정', '배송오류', '배송일정', '배송문의', '배송일정', '배송일정', '배송일정', '상품문의', '배송변경', '상품문의', '상품문의', '배송문의', '취소해지', '미배송', '이벤트', '배송상품', '취소해지', '배송오류', '배송일정', '배송일정', '상품문의', '상품문의', '배송일정', '취소해지', '상품불량']\n",
      "1 미배송 0\n",
      "2 배송일정 1\n",
      "3 미배송 0\n",
      "4 배송장소 2\n",
      "5 미배송 0\n",
      "6 배송일정 1\n",
      "7 배송방법 3\n",
      "8 주문 4\n",
      "9 배송방법 3\n",
      "10 배송방법 3\n",
      "11 배송중단 5\n",
      "12 구매문의 6\n",
      "13 미배송 0\n",
      "14 배송연기 7\n",
      "15 배송오류 8\n",
      "16 배송일정 1\n",
      "17 가방 9\n",
      "18 배송장소 2\n",
      "19 미배송 0\n",
      "20 미배송 0\n",
      "21 배송변경 10\n",
      "22 가방 9\n",
      "23 배송일정 1\n",
      "24 결제오류 11\n",
      "25 결제오류 11\n",
      "26 상품문의 12\n",
      "27 인사 13\n",
      "28 배송일정 1\n",
      "29 배송신청 14\n",
      "30 배송일정 1\n",
      "31 배송장소 2\n",
      "32 상품변경 15\n",
      "33 배송장소 2\n",
      "34 상품문의 12\n",
      "35 상품문의 12\n",
      "36 상품변경 15\n",
      "37 배송문의 16\n",
      "38 미배송 0\n",
      "39 미배송 0\n",
      "40 교환환불 17\n",
      "41 이벤트 18\n",
      "42 시스템 19\n",
      "43 상품문의 12\n",
      "44 상품문의 12\n",
      "45 상품문의 12\n",
      "46 가방 9\n",
      "47 이벤트 18\n",
      "48 가방 9\n",
      "49 배송일정 1\n",
      "50 배송일정 1\n",
      "51 배송상품 20\n",
      "52 배송일정 1\n",
      "53 상품불량 21\n",
      "54 배송문의 16\n",
      "55 배송일정 1\n",
      "56 배송오류 8\n",
      "57 배송일정 1\n",
      "58 배송일정 1\n",
      "59 가방 9\n",
      "60 교환환불 17\n",
      "61 주문변경 22\n",
      "62 상품문의 12\n",
      "63 배송장소 2\n",
      "64 배송일정 1\n",
      "65 배송장소 2\n",
      "66 배송방법 3\n",
      "67 취소해지 23\n",
      "68 취소해지 23\n",
      "69 배송장소 2\n",
      "70 배송장소 2\n",
      "71 교환환불 17\n",
      "72 상품문의 12\n",
      "73 배송일정 1\n",
      "74 배송일정 1\n",
      "75 상품문의 12\n",
      "76 상품불량 21\n",
      "77 상품문의 12\n",
      "78 배송오류 8\n",
      "79 배송오류 8\n",
      "80 배송일정 1\n",
      "81 주문변경 22\n",
      "82 가방 9\n",
      "83 주문 4\n",
      "84 가방 9\n",
      "85 미배송 0\n",
      "86 미배송 0\n",
      "87 상품문의 12\n",
      "88 배송일정 1\n",
      "89 가방 9\n",
      "90 이벤트 18\n",
      "91 배송오류 8\n",
      "92 이벤트 18\n",
      "93 배송일정 1\n",
      "94 취소해지 23\n",
      "95 배송일정 1\n",
      "96 가방 9\n",
      "97 가방 9\n",
      "98 배송상품 20\n",
      "99 상품불량 21\n",
      "100 배송일정 1\n",
      "101 배송상품 20\n",
      "102 배송일정 1\n",
      "103 담당자 24\n",
      "104 가방 9\n",
      "105 배송일정 1\n",
      "106 가방 9\n",
      "107 배송일정 1\n",
      "108 배송오류 8\n",
      "109 미배송 0\n",
      "110 배송일정 1\n",
      "111 상품불량 21\n",
      "112 서비스 25\n",
      "113 배송일정 1\n",
      "114 취소해지 23\n",
      "115 배송장소 2\n",
      "116 상품문의 12\n",
      "117 취소해지 23\n",
      "118 배송장소 2\n",
      "119 가방 9\n",
      "120 상품문의 12\n",
      "121 상품문의 12\n",
      "122 상품문의 12\n",
      "123 취소해지 23\n",
      "124 취소해지 23\n",
      "125 이벤트 18\n",
      "126 담당자 24\n",
      "127 담당자 24\n",
      "128 상품문의 12\n",
      "129 이벤트 18\n",
      "130 배송일정 1\n",
      "131 배송일정 1\n",
      "132 배송일정 1\n",
      "133 취소해지 23\n",
      "134 미배송 0\n",
      "135 배송일정 1\n",
      "136 배송오류 8\n",
      "137 배송일정 1\n",
      "138 배송변경 10\n",
      "139 배송변경 10\n",
      "140 이벤트 18\n",
      "141 배송일정 1\n",
      "142 주문 4\n",
      "143 상품문의 12\n",
      "144 배송문의 16\n",
      "145 상품문의 12\n",
      "146 상품문의 12\n",
      "147 미배송 0\n",
      "148 상품문의 12\n",
      "149 배송일정 1\n",
      "150 이벤트 18\n",
      "151 미배송 0\n",
      "152 배송일정 1\n",
      "153 배송일정 1\n",
      "154 구매문의 6\n",
      "155 구매문의 6\n",
      "156 배송오류 8\n",
      "157 배송일정 1\n",
      "158 배송일정 1\n",
      "159 배송문의 16\n",
      "160 배송오류 8\n",
      "161 배송변경 10\n",
      "162 배송일정 1\n",
      "163 미배송 0\n",
      "164 취소해지 23\n",
      "165 상품불량 21\n",
      "166 상품문의 12\n",
      "167 배송일정 1\n",
      "168 배송변경 10\n",
      "169 배송오류 8\n",
      "170 이벤트 18\n",
      "171 배송변경 10\n",
      "172 배송일정 1\n",
      "173 배송장소 2\n",
      "174 배송장소 2\n",
      "175 배송일정 1\n",
      "176 배송일정 1\n",
      "177 배송오류 8\n",
      "178 배송일정 1\n",
      "179 배송문의 16\n",
      "180 배송일정 1\n",
      "181 배송일정 1\n",
      "182 배송일정 1\n",
      "183 상품문의 12\n",
      "184 배송변경 10\n",
      "185 상품문의 12\n",
      "186 상품문의 12\n",
      "187 배송문의 16\n",
      "188 취소해지 23\n",
      "189 미배송 0\n",
      "190 이벤트 18\n",
      "191 배송상품 20\n",
      "192 취소해지 23\n",
      "193 배송오류 8\n",
      "194 배송일정 1\n",
      "195 배송일정 1\n",
      "196 상품문의 12\n",
      "197 상품문의 12\n",
      "198 배송일정 1\n",
      "199 취소해지 23\n",
      "200 상품불량 21\n"
     ]
    }
   ],
   "source": [
    "cls_idxes = []\n",
    "index = 0\n",
    "\n",
    "for text in cls26:\n",
    "    idx = class_dic[text]\n",
    "    index += 1\n",
    "    print(index , text , idx)\n",
    "    cls_idxes.append(idx)  # cls_idxes : 텍스트에 따른 인덱스들\n",
    "    \n",
    "cls_idxes_arr = np.array(cls_idxes)\n",
    "Y_train_all = cls_idxes_arr\n",
    "# Y_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding , Dropout, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "category = numpy.max(Y_train_all) + 1\n",
    "\n",
    "X_train = X_train_all\n",
    "X_test = X_train_all\n",
    "Y_train = Y_train_all\n",
    "Y_test = Y_train_all\n",
    "print(Y_test.shape)\n",
    "# 데이터 전처리 \n",
    "x_train = sequence.pad_sequences(X_train, maxlen=100) # 200, 100\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=100)  # 200, 100\n",
    "y_train = np_utils.to_categorical(Y_train)  # 200, 26 밑도 마찬가지\n",
    "y_test = np_utils.to_categorical(Y_test)  # categorical data로 변환하는 기술\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.62 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "(200, 100) (200, 100)\n",
      "(200, 26) (200, 26)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.2381 - accuracy: 0.0850 - val_loss: 3.2185 - val_accuracy: 0.2400\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 0s 828us/step - loss: 3.1873 - accuracy: 0.2400 - val_loss: 3.1756 - val_accuracy: 0.2400\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 0s 773us/step - loss: 3.1263 - accuracy: 0.2400 - val_loss: 3.1143 - val_accuracy: 0.2400\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 0s 741us/step - loss: 3.0472 - accuracy: 0.2400 - val_loss: 3.0286 - val_accuracy: 0.2400\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 0s 738us/step - loss: 2.9516 - accuracy: 0.2400 - val_loss: 2.9238 - val_accuracy: 0.2400\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 0s 768us/step - loss: 2.8689 - accuracy: 0.2400 - val_loss: 2.8366 - val_accuracy: 0.2400\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 0s 773us/step - loss: 2.8156 - accuracy: 0.2400 - val_loss: 2.7903 - val_accuracy: 0.2400\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 0s 873us/step - loss: 2.7894 - accuracy: 0.2400 - val_loss: 2.7586 - val_accuracy: 0.2400\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 0s 758us/step - loss: 2.7584 - accuracy: 0.2400 - val_loss: 2.7312 - val_accuracy: 0.2400\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 0s 888us/step - loss: 2.7261 - accuracy: 0.2400 - val_loss: 2.7141 - val_accuracy: 0.2400\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 0s 763us/step - loss: 2.7129 - accuracy: 0.2400 - val_loss: 2.7045 - val_accuracy: 0.2400\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 0s 778us/step - loss: 2.7038 - accuracy: 0.2400 - val_loss: 2.6961 - val_accuracy: 0.2400\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 0s 795us/step - loss: 2.6933 - accuracy: 0.2400 - val_loss: 2.6855 - val_accuracy: 0.2400\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 0s 903us/step - loss: 2.6831 - accuracy: 0.2400 - val_loss: 2.6745 - val_accuracy: 0.2400\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 0s 788us/step - loss: 2.6714 - accuracy: 0.2400 - val_loss: 2.6656 - val_accuracy: 0.2400\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 0s 743us/step - loss: 2.6654 - accuracy: 0.2400 - val_loss: 2.6579 - val_accuracy: 0.2400\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 0s 723us/step - loss: 2.6602 - accuracy: 0.2400 - val_loss: 2.6500 - val_accuracy: 0.2400\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 0s 768us/step - loss: 2.6510 - accuracy: 0.2400 - val_loss: 2.6408 - val_accuracy: 0.2400\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 0s 869us/step - loss: 2.6414 - accuracy: 0.2400 - val_loss: 2.6312 - val_accuracy: 0.2400\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 0s 793us/step - loss: 2.6318 - accuracy: 0.2400 - val_loss: 2.6217 - val_accuracy: 0.2500\n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 0s 808us/step - loss: 2.6236 - accuracy: 0.2600 - val_loss: 2.6126 - val_accuracy: 0.2850\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 0s 855us/step - loss: 2.6128 - accuracy: 0.2650 - val_loss: 2.6007 - val_accuracy: 0.2800\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 0s 733us/step - loss: 2.6026 - accuracy: 0.2700 - val_loss: 2.5868 - val_accuracy: 0.2900\n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 0s 793us/step - loss: 2.5868 - accuracy: 0.2750 - val_loss: 2.5727 - val_accuracy: 0.2800\n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 0s 808us/step - loss: 2.5748 - accuracy: 0.2750 - val_loss: 2.5584 - val_accuracy: 0.2950\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 0s 738us/step - loss: 2.5590 - accuracy: 0.2800 - val_loss: 2.5433 - val_accuracy: 0.2900\n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 0s 808us/step - loss: 2.5493 - accuracy: 0.3000 - val_loss: 2.5281 - val_accuracy: 0.2900\n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 0s 780us/step - loss: 2.5386 - accuracy: 0.2850 - val_loss: 2.5140 - val_accuracy: 0.2800\n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 0s 726us/step - loss: 2.5119 - accuracy: 0.2900 - val_loss: 2.4949 - val_accuracy: 0.2950\n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 0s 746us/step - loss: 2.4908 - accuracy: 0.2850 - val_loss: 2.4737 - val_accuracy: 0.3000\n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 2.4762 - accuracy: 0.3150 - val_loss: 2.4531 - val_accuracy: 0.3050\n",
      "Epoch 32/200\n",
      "200/200 [==============================] - 0s 743us/step - loss: 2.4534 - accuracy: 0.3150 - val_loss: 2.4349 - val_accuracy: 0.3200\n",
      "Epoch 33/200\n",
      "200/200 [==============================] - 0s 728us/step - loss: 2.4340 - accuracy: 0.3250 - val_loss: 2.4212 - val_accuracy: 0.3250\n",
      "Epoch 34/200\n",
      "200/200 [==============================] - 0s 697us/step - loss: 2.4078 - accuracy: 0.3400 - val_loss: 2.4023 - val_accuracy: 0.3300\n",
      "Epoch 35/200\n",
      "200/200 [==============================] - 0s 726us/step - loss: 2.3952 - accuracy: 0.3350 - val_loss: 2.3761 - val_accuracy: 0.3400\n",
      "Epoch 36/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 2.3860 - accuracy: 0.3450 - val_loss: 2.3621 - val_accuracy: 0.3550\n",
      "Epoch 37/200\n",
      "200/200 [==============================] - 0s 723us/step - loss: 2.3581 - accuracy: 0.3600 - val_loss: 2.3459 - val_accuracy: 0.3600\n",
      "Epoch 38/200\n",
      "200/200 [==============================] - 0s 728us/step - loss: 2.3363 - accuracy: 0.3500 - val_loss: 2.3250 - val_accuracy: 0.3600\n",
      "Epoch 39/200\n",
      "200/200 [==============================] - 0s 733us/step - loss: 2.3119 - accuracy: 0.3700 - val_loss: 2.3068 - val_accuracy: 0.3550\n",
      "Epoch 40/200\n",
      "200/200 [==============================] - 0s 763us/step - loss: 2.2965 - accuracy: 0.3750 - val_loss: 2.3015 - val_accuracy: 0.3400\n",
      "Epoch 41/200\n",
      "200/200 [==============================] - 0s 771us/step - loss: 2.2794 - accuracy: 0.3800 - val_loss: 2.2727 - val_accuracy: 0.3450\n",
      "Epoch 42/200\n",
      "200/200 [==============================] - 0s 752us/step - loss: 2.2747 - accuracy: 0.3500 - val_loss: 2.2425 - val_accuracy: 0.3650\n",
      "Epoch 43/200\n",
      "200/200 [==============================] - 0s 808us/step - loss: 2.2533 - accuracy: 0.3650 - val_loss: 2.2561 - val_accuracy: 0.3450\n",
      "Epoch 44/200\n",
      "200/200 [==============================] - 0s 868us/step - loss: 2.2254 - accuracy: 0.3550 - val_loss: 2.2234 - val_accuracy: 0.3450\n",
      "Epoch 45/200\n",
      "200/200 [==============================] - 0s 780us/step - loss: 2.2084 - accuracy: 0.3750 - val_loss: 2.1720 - val_accuracy: 0.3750\n",
      "Epoch 46/200\n",
      "200/200 [==============================] - 0s 733us/step - loss: 2.1722 - accuracy: 0.3800 - val_loss: 2.1582 - val_accuracy: 0.3650\n",
      "Epoch 47/200\n",
      "200/200 [==============================] - 0s 738us/step - loss: 2.1680 - accuracy: 0.3750 - val_loss: 2.1587 - val_accuracy: 0.3600\n",
      "Epoch 48/200\n",
      "200/200 [==============================] - 0s 728us/step - loss: 2.1553 - accuracy: 0.3700 - val_loss: 2.1353 - val_accuracy: 0.3750\n",
      "Epoch 49/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 2.1249 - accuracy: 0.3850 - val_loss: 2.0917 - val_accuracy: 0.3950\n",
      "Epoch 50/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 2.1045 - accuracy: 0.3900 - val_loss: 2.0650 - val_accuracy: 0.3900\n",
      "Epoch 51/200\n",
      "200/200 [==============================] - 0s 738us/step - loss: 2.0716 - accuracy: 0.3950 - val_loss: 2.0493 - val_accuracy: 0.4100\n",
      "Epoch 52/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 2.0485 - accuracy: 0.3900 - val_loss: 2.0568 - val_accuracy: 0.4150\n",
      "Epoch 53/200\n",
      "200/200 [==============================] - 0s 768us/step - loss: 2.0167 - accuracy: 0.4050 - val_loss: 2.0282 - val_accuracy: 0.4200\n",
      "Epoch 54/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 2.0131 - accuracy: 0.4100 - val_loss: 1.9823 - val_accuracy: 0.4300\n",
      "Epoch 55/200\n",
      "200/200 [==============================] - 0s 780us/step - loss: 1.9989 - accuracy: 0.4050 - val_loss: 1.9560 - val_accuracy: 0.4350\n",
      "Epoch 56/200\n",
      "200/200 [==============================] - 0s 701us/step - loss: 1.9808 - accuracy: 0.4050 - val_loss: 1.9351 - val_accuracy: 0.4450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 1.9316 - accuracy: 0.4200 - val_loss: 1.9295 - val_accuracy: 0.4450\n",
      "Epoch 58/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 1.9341 - accuracy: 0.4250 - val_loss: 1.9081 - val_accuracy: 0.4450\n",
      "Epoch 59/200\n",
      "200/200 [==============================] - 0s 758us/step - loss: 1.8943 - accuracy: 0.4250 - val_loss: 1.8726 - val_accuracy: 0.4450\n",
      "Epoch 60/200\n",
      "200/200 [==============================] - 0s 718us/step - loss: 1.8811 - accuracy: 0.4400 - val_loss: 1.8425 - val_accuracy: 0.4550\n",
      "Epoch 61/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 1.8536 - accuracy: 0.4400 - val_loss: 1.8169 - val_accuracy: 0.4600\n",
      "Epoch 62/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 1.8351 - accuracy: 0.4350 - val_loss: 1.8237 - val_accuracy: 0.4650\n",
      "Epoch 63/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 1.8181 - accuracy: 0.4700 - val_loss: 1.7796 - val_accuracy: 0.4800\n",
      "Epoch 64/200\n",
      "200/200 [==============================] - 0s 696us/step - loss: 1.7798 - accuracy: 0.4900 - val_loss: 1.7398 - val_accuracy: 0.4850\n",
      "Epoch 65/200\n",
      "200/200 [==============================] - 0s 770us/step - loss: 1.7621 - accuracy: 0.4600 - val_loss: 1.7142 - val_accuracy: 0.5200\n",
      "Epoch 66/200\n",
      "200/200 [==============================] - 0s 715us/step - loss: 1.7496 - accuracy: 0.4850 - val_loss: 1.6972 - val_accuracy: 0.5200\n",
      "Epoch 67/200\n",
      "200/200 [==============================] - 0s 702us/step - loss: 1.7110 - accuracy: 0.4900 - val_loss: 1.6816 - val_accuracy: 0.5300\n",
      "Epoch 68/200\n",
      "200/200 [==============================] - 0s 952us/step - loss: 1.6839 - accuracy: 0.5200 - val_loss: 1.6525 - val_accuracy: 0.5350\n",
      "Epoch 69/200\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.6746 - accuracy: 0.5250 - val_loss: 1.6210 - val_accuracy: 0.5350\n",
      "Epoch 70/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 1.6451 - accuracy: 0.5250 - val_loss: 1.6031 - val_accuracy: 0.5500\n",
      "Epoch 71/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 1.6023 - accuracy: 0.5250 - val_loss: 1.5775 - val_accuracy: 0.5550\n",
      "Epoch 72/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 1.6095 - accuracy: 0.5450 - val_loss: 1.5593 - val_accuracy: 0.5700\n",
      "Epoch 73/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 1.5836 - accuracy: 0.5500 - val_loss: 1.5288 - val_accuracy: 0.5700\n",
      "Epoch 74/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 1.5633 - accuracy: 0.5500 - val_loss: 1.4957 - val_accuracy: 0.5750\n",
      "Epoch 75/200\n",
      "200/200 [==============================] - 0s 666us/step - loss: 1.5197 - accuracy: 0.5700 - val_loss: 1.4692 - val_accuracy: 0.5900\n",
      "Epoch 76/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 1.5153 - accuracy: 0.5550 - val_loss: 1.4522 - val_accuracy: 0.5850\n",
      "Epoch 77/200\n",
      "200/200 [==============================] - 0s 706us/step - loss: 1.4973 - accuracy: 0.5750 - val_loss: 1.4295 - val_accuracy: 0.5950\n",
      "Epoch 78/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 1.4548 - accuracy: 0.5750 - val_loss: 1.4008 - val_accuracy: 0.6000\n",
      "Epoch 79/200\n",
      "200/200 [==============================] - 0s 763us/step - loss: 1.4667 - accuracy: 0.5600 - val_loss: 1.3707 - val_accuracy: 0.6300\n",
      "Epoch 80/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 1.4289 - accuracy: 0.5750 - val_loss: 1.3597 - val_accuracy: 0.6250\n",
      "Epoch 81/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 1.4031 - accuracy: 0.5850 - val_loss: 1.3405 - val_accuracy: 0.6150\n",
      "Epoch 82/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 1.3716 - accuracy: 0.6000 - val_loss: 1.3214 - val_accuracy: 0.6300\n",
      "Epoch 83/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 1.3525 - accuracy: 0.6250 - val_loss: 1.2948 - val_accuracy: 0.6350\n",
      "Epoch 84/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 1.3449 - accuracy: 0.6100 - val_loss: 1.2653 - val_accuracy: 0.6800\n",
      "Epoch 85/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 1.3126 - accuracy: 0.6400 - val_loss: 1.2396 - val_accuracy: 0.6800\n",
      "Epoch 86/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 1.3036 - accuracy: 0.6600 - val_loss: 1.2301 - val_accuracy: 0.6700\n",
      "Epoch 87/200\n",
      "200/200 [==============================] - 0s 677us/step - loss: 1.2754 - accuracy: 0.6450 - val_loss: 1.2139 - val_accuracy: 0.6800\n",
      "Epoch 88/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 1.2897 - accuracy: 0.6450 - val_loss: 1.1837 - val_accuracy: 0.6900\n",
      "Epoch 89/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 1.2419 - accuracy: 0.6400 - val_loss: 1.1666 - val_accuracy: 0.6950\n",
      "Epoch 90/200\n",
      "200/200 [==============================] - 0s 740us/step - loss: 1.2285 - accuracy: 0.6700 - val_loss: 1.1470 - val_accuracy: 0.7100\n",
      "Epoch 91/200\n",
      "200/200 [==============================] - 0s 701us/step - loss: 1.2034 - accuracy: 0.6750 - val_loss: 1.1210 - val_accuracy: 0.7300\n",
      "Epoch 92/200\n",
      "200/200 [==============================] - 0s 733us/step - loss: 1.1707 - accuracy: 0.7100 - val_loss: 1.1005 - val_accuracy: 0.7250\n",
      "Epoch 93/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 1.1520 - accuracy: 0.7000 - val_loss: 1.0839 - val_accuracy: 0.7350\n",
      "Epoch 94/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 1.1585 - accuracy: 0.7100 - val_loss: 1.0717 - val_accuracy: 0.7350\n",
      "Epoch 95/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 1.1528 - accuracy: 0.6750 - val_loss: 1.0448 - val_accuracy: 0.7500\n",
      "Epoch 96/200\n",
      "200/200 [==============================] - 0s 718us/step - loss: 1.1117 - accuracy: 0.7150 - val_loss: 1.0277 - val_accuracy: 0.7550\n",
      "Epoch 97/200\n",
      "200/200 [==============================] - 0s 711us/step - loss: 1.0807 - accuracy: 0.7350 - val_loss: 1.0104 - val_accuracy: 0.7650\n",
      "Epoch 98/200\n",
      "200/200 [==============================] - 0s 715us/step - loss: 1.0795 - accuracy: 0.7200 - val_loss: 0.9983 - val_accuracy: 0.7600\n",
      "Epoch 99/200\n",
      "200/200 [==============================] - 0s 723us/step - loss: 1.0655 - accuracy: 0.7150 - val_loss: 0.9833 - val_accuracy: 0.7600\n",
      "Epoch 100/200\n",
      "200/200 [==============================] - 0s 713us/step - loss: 1.0519 - accuracy: 0.7300 - val_loss: 0.9641 - val_accuracy: 0.7650\n",
      "Epoch 101/200\n",
      "200/200 [==============================] - 0s 695us/step - loss: 1.0374 - accuracy: 0.7300 - val_loss: 0.9370 - val_accuracy: 0.7700\n",
      "Epoch 102/200\n",
      "200/200 [==============================] - 0s 685us/step - loss: 1.0161 - accuracy: 0.7350 - val_loss: 0.9221 - val_accuracy: 0.7800\n",
      "Epoch 103/200\n",
      "200/200 [==============================] - 0s 671us/step - loss: 1.0030 - accuracy: 0.7450 - val_loss: 0.9112 - val_accuracy: 0.7800\n",
      "Epoch 104/200\n",
      "200/200 [==============================] - 0s 713us/step - loss: 0.9727 - accuracy: 0.7650 - val_loss: 0.8960 - val_accuracy: 0.7850\n",
      "Epoch 105/200\n",
      "200/200 [==============================] - 0s 713us/step - loss: 0.9902 - accuracy: 0.7400 - val_loss: 0.8786 - val_accuracy: 0.7850\n",
      "Epoch 106/200\n",
      "200/200 [==============================] - 0s 748us/step - loss: 0.9662 - accuracy: 0.7600 - val_loss: 0.8603 - val_accuracy: 0.7900\n",
      "Epoch 107/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 0.9562 - accuracy: 0.7600 - val_loss: 0.8497 - val_accuracy: 0.7900\n",
      "Epoch 108/200\n",
      "200/200 [==============================] - 0s 853us/step - loss: 0.9363 - accuracy: 0.7550 - val_loss: 0.8313 - val_accuracy: 0.7950\n",
      "Epoch 109/200\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.9073 - accuracy: 0.7900 - val_loss: 0.8213 - val_accuracy: 0.8000\n",
      "Epoch 110/200\n",
      "200/200 [==============================] - 0s 681us/step - loss: 0.9020 - accuracy: 0.7750 - val_loss: 0.8038 - val_accuracy: 0.8050\n",
      "Epoch 111/200\n",
      "200/200 [==============================] - 0s 751us/step - loss: 0.8829 - accuracy: 0.7800 - val_loss: 0.7837 - val_accuracy: 0.8150\n",
      "Epoch 112/200\n",
      "200/200 [==============================] - 0s 749us/step - loss: 0.8450 - accuracy: 0.7850 - val_loss: 0.7721 - val_accuracy: 0.8150\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 778us/step - loss: 0.8786 - accuracy: 0.8000 - val_loss: 0.7688 - val_accuracy: 0.8100\n",
      "Epoch 114/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 0.8475 - accuracy: 0.8000 - val_loss: 0.7699 - val_accuracy: 0.8050\n",
      "Epoch 115/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.8336 - accuracy: 0.8200 - val_loss: 0.7314 - val_accuracy: 0.8350\n",
      "Epoch 116/200\n",
      "200/200 [==============================] - 0s 681us/step - loss: 0.8248 - accuracy: 0.7900 - val_loss: 0.7109 - val_accuracy: 0.8450\n",
      "Epoch 117/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.8161 - accuracy: 0.8200 - val_loss: 0.7066 - val_accuracy: 0.8450\n",
      "Epoch 118/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.8123 - accuracy: 0.8100 - val_loss: 0.7009 - val_accuracy: 0.8450\n",
      "Epoch 119/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.7740 - accuracy: 0.8150 - val_loss: 0.6960 - val_accuracy: 0.8450\n",
      "Epoch 120/200\n",
      "200/200 [==============================] - 0s 728us/step - loss: 0.7596 - accuracy: 0.8300 - val_loss: 0.6820 - val_accuracy: 0.8400\n",
      "Epoch 121/200\n",
      "200/200 [==============================] - 0s 708us/step - loss: 0.7769 - accuracy: 0.8100 - val_loss: 0.6615 - val_accuracy: 0.8600\n",
      "Epoch 122/200\n",
      "200/200 [==============================] - 0s 731us/step - loss: 0.7474 - accuracy: 0.8150 - val_loss: 0.6525 - val_accuracy: 0.8700\n",
      "Epoch 123/200\n",
      "200/200 [==============================] - 0s 712us/step - loss: 0.7545 - accuracy: 0.8250 - val_loss: 0.6480 - val_accuracy: 0.8750\n",
      "Epoch 124/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.7214 - accuracy: 0.8200 - val_loss: 0.6272 - val_accuracy: 0.8750\n",
      "Epoch 125/200\n",
      "200/200 [==============================] - 0s 748us/step - loss: 0.7092 - accuracy: 0.8250 - val_loss: 0.6122 - val_accuracy: 0.8850\n",
      "Epoch 126/200\n",
      "200/200 [==============================] - 0s 723us/step - loss: 0.7284 - accuracy: 0.8500 - val_loss: 0.5997 - val_accuracy: 0.8900\n",
      "Epoch 127/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.6997 - accuracy: 0.8200 - val_loss: 0.5931 - val_accuracy: 0.8900\n",
      "Epoch 128/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 0.7002 - accuracy: 0.8500 - val_loss: 0.5908 - val_accuracy: 0.8900\n",
      "Epoch 129/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.6772 - accuracy: 0.8600 - val_loss: 0.5823 - val_accuracy: 0.8900\n",
      "Epoch 130/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.6719 - accuracy: 0.8350 - val_loss: 0.5643 - val_accuracy: 0.8950\n",
      "Epoch 131/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.6709 - accuracy: 0.8650 - val_loss: 0.5521 - val_accuracy: 0.8950\n",
      "Epoch 132/200\n",
      "200/200 [==============================] - 0s 728us/step - loss: 0.6320 - accuracy: 0.8700 - val_loss: 0.5417 - val_accuracy: 0.8950\n",
      "Epoch 133/200\n",
      "200/200 [==============================] - 0s 733us/step - loss: 0.6595 - accuracy: 0.8750 - val_loss: 0.5298 - val_accuracy: 0.8950\n",
      "Epoch 134/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.6423 - accuracy: 0.8550 - val_loss: 0.5262 - val_accuracy: 0.8900\n",
      "Epoch 135/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.6131 - accuracy: 0.8700 - val_loss: 0.5233 - val_accuracy: 0.8950\n",
      "Epoch 136/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 0.6378 - accuracy: 0.8450 - val_loss: 0.5094 - val_accuracy: 0.8950\n",
      "Epoch 137/200\n",
      "200/200 [==============================] - 0s 696us/step - loss: 0.6035 - accuracy: 0.8700 - val_loss: 0.4982 - val_accuracy: 0.9100\n",
      "Epoch 138/200\n",
      "200/200 [==============================] - 0s 701us/step - loss: 0.5939 - accuracy: 0.8850 - val_loss: 0.4921 - val_accuracy: 0.9100\n",
      "Epoch 139/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.5938 - accuracy: 0.8850 - val_loss: 0.4901 - val_accuracy: 0.9000\n",
      "Epoch 140/200\n",
      "200/200 [==============================] - 0s 708us/step - loss: 0.5682 - accuracy: 0.8800 - val_loss: 0.4798 - val_accuracy: 0.9000\n",
      "Epoch 141/200\n",
      "200/200 [==============================] - 0s 713us/step - loss: 0.5846 - accuracy: 0.8800 - val_loss: 0.4690 - val_accuracy: 0.9050\n",
      "Epoch 142/200\n",
      "200/200 [==============================] - 0s 718us/step - loss: 0.5642 - accuracy: 0.8750 - val_loss: 0.4611 - val_accuracy: 0.9000\n",
      "Epoch 143/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.5531 - accuracy: 0.8700 - val_loss: 0.4587 - val_accuracy: 0.9100\n",
      "Epoch 144/200\n",
      "200/200 [==============================] - 0s 696us/step - loss: 0.5396 - accuracy: 0.9000 - val_loss: 0.4532 - val_accuracy: 0.9100\n",
      "Epoch 145/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.5447 - accuracy: 0.8850 - val_loss: 0.4421 - val_accuracy: 0.9150\n",
      "Epoch 146/200\n",
      "200/200 [==============================] - 0s 833us/step - loss: 0.5565 - accuracy: 0.9000 - val_loss: 0.4257 - val_accuracy: 0.9200\n",
      "Epoch 147/200\n",
      "200/200 [==============================] - 0s 843us/step - loss: 0.5081 - accuracy: 0.9000 - val_loss: 0.4238 - val_accuracy: 0.9150\n",
      "Epoch 148/200\n",
      "200/200 [==============================] - 0s 721us/step - loss: 0.5360 - accuracy: 0.8850 - val_loss: 0.4229 - val_accuracy: 0.9100\n",
      "Epoch 149/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 0.5133 - accuracy: 0.8950 - val_loss: 0.4160 - val_accuracy: 0.9100\n",
      "Epoch 150/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.4977 - accuracy: 0.9000 - val_loss: 0.4060 - val_accuracy: 0.9300\n",
      "Epoch 151/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 0.5078 - accuracy: 0.9000 - val_loss: 0.3915 - val_accuracy: 0.9250\n",
      "Epoch 152/200\n",
      "200/200 [==============================] - 0s 718us/step - loss: 0.5061 - accuracy: 0.8900 - val_loss: 0.3843 - val_accuracy: 0.9250\n",
      "Epoch 153/200\n",
      "200/200 [==============================] - 0s 708us/step - loss: 0.5064 - accuracy: 0.8900 - val_loss: 0.3846 - val_accuracy: 0.9150\n",
      "Epoch 154/200\n",
      "200/200 [==============================] - 0s 783us/step - loss: 0.4722 - accuracy: 0.9200 - val_loss: 0.3831 - val_accuracy: 0.9150\n",
      "Epoch 155/200\n",
      "200/200 [==============================] - 0s 793us/step - loss: 0.4821 - accuracy: 0.8950 - val_loss: 0.3763 - val_accuracy: 0.9250\n",
      "Epoch 156/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.4750 - accuracy: 0.9050 - val_loss: 0.3676 - val_accuracy: 0.9350\n",
      "Epoch 157/200\n",
      "200/200 [==============================] - 0s 698us/step - loss: 0.4581 - accuracy: 0.9200 - val_loss: 0.3584 - val_accuracy: 0.9400\n",
      "Epoch 158/200\n",
      "200/200 [==============================] - 0s 696us/step - loss: 0.4675 - accuracy: 0.9050 - val_loss: 0.3603 - val_accuracy: 0.9300\n",
      "Epoch 159/200\n",
      "200/200 [==============================] - 0s 711us/step - loss: 0.4461 - accuracy: 0.9100 - val_loss: 0.3546 - val_accuracy: 0.9300\n",
      "Epoch 160/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.4424 - accuracy: 0.9200 - val_loss: 0.3438 - val_accuracy: 0.9350\n",
      "Epoch 161/200\n",
      "200/200 [==============================] - 0s 708us/step - loss: 0.4306 - accuracy: 0.9300 - val_loss: 0.3371 - val_accuracy: 0.9350\n",
      "Epoch 162/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 0.4361 - accuracy: 0.9300 - val_loss: 0.3275 - val_accuracy: 0.9400\n",
      "Epoch 163/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 0.4306 - accuracy: 0.9100 - val_loss: 0.3285 - val_accuracy: 0.9400\n",
      "Epoch 164/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 0.4277 - accuracy: 0.9100 - val_loss: 0.3202 - val_accuracy: 0.9400\n",
      "Epoch 165/200\n",
      "200/200 [==============================] - 0s 741us/step - loss: 0.4184 - accuracy: 0.9200 - val_loss: 0.3167 - val_accuracy: 0.9400\n",
      "Epoch 166/200\n",
      "200/200 [==============================] - 0s 738us/step - loss: 0.4423 - accuracy: 0.8900 - val_loss: 0.3158 - val_accuracy: 0.9400\n",
      "Epoch 167/200\n",
      "200/200 [==============================] - 0s 691us/step - loss: 0.4040 - accuracy: 0.9150 - val_loss: 0.3049 - val_accuracy: 0.9400\n",
      "Epoch 168/200\n",
      "200/200 [==============================] - 0s 758us/step - loss: 0.3921 - accuracy: 0.9200 - val_loss: 0.3001 - val_accuracy: 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "200/200 [==============================] - 0s 719us/step - loss: 0.4059 - accuracy: 0.9150 - val_loss: 0.2980 - val_accuracy: 0.9400\n",
      "Epoch 170/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.3963 - accuracy: 0.9350 - val_loss: 0.2885 - val_accuracy: 0.9450\n",
      "Epoch 171/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.3739 - accuracy: 0.9350 - val_loss: 0.2841 - val_accuracy: 0.9400\n",
      "Epoch 172/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.4014 - accuracy: 0.8950 - val_loss: 0.2838 - val_accuracy: 0.9400\n",
      "Epoch 173/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 0.3826 - accuracy: 0.9250 - val_loss: 0.2787 - val_accuracy: 0.9400\n",
      "Epoch 174/200\n",
      "200/200 [==============================] - 0s 718us/step - loss: 0.3665 - accuracy: 0.9400 - val_loss: 0.2750 - val_accuracy: 0.9450\n",
      "Epoch 175/200\n",
      "200/200 [==============================] - 0s 778us/step - loss: 0.3673 - accuracy: 0.9250 - val_loss: 0.2706 - val_accuracy: 0.9500\n",
      "Epoch 176/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.3478 - accuracy: 0.9400 - val_loss: 0.2644 - val_accuracy: 0.9500\n",
      "Epoch 177/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.3631 - accuracy: 0.9400 - val_loss: 0.2578 - val_accuracy: 0.9450\n",
      "Epoch 178/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.3426 - accuracy: 0.9350 - val_loss: 0.2541 - val_accuracy: 0.9450\n",
      "Epoch 179/200\n",
      "200/200 [==============================] - 0s 677us/step - loss: 0.3366 - accuracy: 0.9350 - val_loss: 0.2525 - val_accuracy: 0.9450\n",
      "Epoch 180/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.3260 - accuracy: 0.9400 - val_loss: 0.2489 - val_accuracy: 0.9450\n",
      "Epoch 181/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.3173 - accuracy: 0.9450 - val_loss: 0.2452 - val_accuracy: 0.9450\n",
      "Epoch 182/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.3156 - accuracy: 0.9400 - val_loss: 0.2437 - val_accuracy: 0.9450\n",
      "Epoch 183/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.3136 - accuracy: 0.9450 - val_loss: 0.2437 - val_accuracy: 0.9500\n",
      "Epoch 184/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 0.3178 - accuracy: 0.9350 - val_loss: 0.2397 - val_accuracy: 0.9600\n",
      "Epoch 185/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.3220 - accuracy: 0.9350 - val_loss: 0.2318 - val_accuracy: 0.9600\n",
      "Epoch 186/200\n",
      "200/200 [==============================] - 0s 676us/step - loss: 0.2987 - accuracy: 0.9450 - val_loss: 0.2267 - val_accuracy: 0.9600\n",
      "Epoch 187/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.2914 - accuracy: 0.9450 - val_loss: 0.2220 - val_accuracy: 0.9550\n",
      "Epoch 188/200\n",
      "200/200 [==============================] - 0s 685us/step - loss: 0.3031 - accuracy: 0.9400 - val_loss: 0.2175 - val_accuracy: 0.9600\n",
      "Epoch 189/200\n",
      "200/200 [==============================] - 0s 693us/step - loss: 0.2974 - accuracy: 0.9550 - val_loss: 0.2161 - val_accuracy: 0.9650\n",
      "Epoch 190/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.2890 - accuracy: 0.9550 - val_loss: 0.2157 - val_accuracy: 0.9650\n",
      "Epoch 191/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 0.2858 - accuracy: 0.9400 - val_loss: 0.2133 - val_accuracy: 0.9600\n",
      "Epoch 192/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.2869 - accuracy: 0.9400 - val_loss: 0.2090 - val_accuracy: 0.9600\n",
      "Epoch 193/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.2895 - accuracy: 0.9350 - val_loss: 0.2031 - val_accuracy: 0.9650\n",
      "Epoch 194/200\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.2972 - accuracy: 0.9500 - val_loss: 0.1991 - val_accuracy: 0.9650\n",
      "Epoch 195/200\n",
      "200/200 [==============================] - 0s 703us/step - loss: 0.2868 - accuracy: 0.9500 - val_loss: 0.1997 - val_accuracy: 0.9750\n",
      "Epoch 196/200\n",
      "200/200 [==============================] - 0s 678us/step - loss: 0.2786 - accuracy: 0.9400 - val_loss: 0.1998 - val_accuracy: 0.9650\n",
      "Epoch 197/200\n",
      "200/200 [==============================] - 0s 748us/step - loss: 0.2828 - accuracy: 0.9500 - val_loss: 0.1998 - val_accuracy: 0.9650\n",
      "Epoch 198/200\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.2676 - accuracy: 0.9500 - val_loss: 0.1980 - val_accuracy: 0.9650\n",
      "Epoch 199/200\n",
      "200/200 [==============================] - 0s 723us/step - loss: 0.2864 - accuracy: 0.9350 - val_loss: 0.1926 - val_accuracy: 0.9700\n",
      "Epoch 200/200\n",
      "200/200 [==============================] - 0s 688us/step - loss: 0.2714 - accuracy: 0.9500 - val_loss: 0.1853 - val_accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(970, 100))\n",
    "model.add(Dropout(0.62))\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(26, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(y_train)\n",
    "history = model.fit(x_train, y_train, batch_size=100, epochs=200, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 827us/step\n",
      "\n",
      " Test Accuracy: 0.9700\n",
      "200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\n Test Accuracy: {0:0.4f}'.format(model.evaluate(x_test, y_test)[1]))\n",
    "\n",
    "y_vloss = history.history['val_loss'] # ephocs를 돌면서 나온 vloss의 기록을 의미\n",
    "print(len(y_vloss)) # 200개, \n",
    "x_len = np.arange(len(y_vloss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.62 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(970, 100))\n",
    "model.add(Dropout(0.62))\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(26, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(\n",
    "    x_train, y_train, test_size=0.33, random_state=0\n",
    ")\n",
    "print(len(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 134 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 3.2460 - accuracy: 0.0299 - val_loss: 3.2239 - val_accuracy: 0.2400\n",
      "Epoch 2/200\n",
      "134/134 [==============================] - 0s 990us/step - loss: 3.1930 - accuracy: 0.2313 - val_loss: 3.1807 - val_accuracy: 0.2400\n",
      "Epoch 3/200\n",
      "134/134 [==============================] - 0s 968us/step - loss: 3.1317 - accuracy: 0.2463 - val_loss: 3.1206 - val_accuracy: 0.2400\n",
      "Epoch 4/200\n",
      "134/134 [==============================] - 0s 871us/step - loss: 3.0532 - accuracy: 0.2463 - val_loss: 3.0364 - val_accuracy: 0.2400\n",
      "Epoch 5/200\n",
      "134/134 [==============================] - 0s 815us/step - loss: 2.9568 - accuracy: 0.2463 - val_loss: 2.9382 - val_accuracy: 0.2400\n",
      "Epoch 6/200\n",
      "134/134 [==============================] - 0s 871us/step - loss: 2.8747 - accuracy: 0.2463 - val_loss: 2.8564 - val_accuracy: 0.2400\n",
      "Epoch 7/200\n",
      "134/134 [==============================] - 0s 930us/step - loss: 2.8093 - accuracy: 0.2463 - val_loss: 2.8106 - val_accuracy: 0.2400\n",
      "Epoch 8/200\n",
      "134/134 [==============================] - 0s 960us/step - loss: 2.7787 - accuracy: 0.2463 - val_loss: 2.7881 - val_accuracy: 0.2400\n",
      "Epoch 9/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.7640 - accuracy: 0.2463 - val_loss: 2.7667 - val_accuracy: 0.2400\n",
      "Epoch 10/200\n",
      "134/134 [==============================] - 0s 997us/step - loss: 2.7373 - accuracy: 0.2463 - val_loss: 2.7444 - val_accuracy: 0.2400\n",
      "Epoch 11/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.7113 - accuracy: 0.2463 - val_loss: 2.7243 - val_accuracy: 0.2400\n",
      "Epoch 12/200\n",
      "134/134 [==============================] - 0s 945us/step - loss: 2.6897 - accuracy: 0.2463 - val_loss: 2.7096 - val_accuracy: 0.2400\n",
      "Epoch 13/200\n",
      "134/134 [==============================] - 0s 908us/step - loss: 2.6704 - accuracy: 0.2463 - val_loss: 2.6998 - val_accuracy: 0.2400\n",
      "Epoch 14/200\n",
      "134/134 [==============================] - 0s 901us/step - loss: 2.6622 - accuracy: 0.2463 - val_loss: 2.6934 - val_accuracy: 0.2400\n",
      "Epoch 15/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 2.6534 - accuracy: 0.2463 - val_loss: 2.6877 - val_accuracy: 0.2400\n",
      "Epoch 16/200\n",
      "134/134 [==============================] - 0s 949us/step - loss: 2.6443 - accuracy: 0.2463 - val_loss: 2.6809 - val_accuracy: 0.2400\n",
      "Epoch 17/200\n",
      "134/134 [==============================] - 0s 908us/step - loss: 2.6337 - accuracy: 0.2463 - val_loss: 2.6737 - val_accuracy: 0.2400\n",
      "Epoch 18/200\n",
      "134/134 [==============================] - 0s 901us/step - loss: 2.6227 - accuracy: 0.2463 - val_loss: 2.6662 - val_accuracy: 0.2400\n",
      "Epoch 19/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.6140 - accuracy: 0.2463 - val_loss: 2.6602 - val_accuracy: 0.2400\n",
      "Epoch 20/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.5999 - accuracy: 0.2463 - val_loss: 2.6540 - val_accuracy: 0.2400\n",
      "Epoch 21/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.5924 - accuracy: 0.2463 - val_loss: 2.6486 - val_accuracy: 0.2400\n",
      "Epoch 22/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.5818 - accuracy: 0.2463 - val_loss: 2.6434 - val_accuracy: 0.2400\n",
      "Epoch 23/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.5767 - accuracy: 0.2463 - val_loss: 2.6388 - val_accuracy: 0.2400\n",
      "Epoch 24/200\n",
      "134/134 [==============================] - 0s 962us/step - loss: 2.5635 - accuracy: 0.2463 - val_loss: 2.6348 - val_accuracy: 0.2400\n",
      "Epoch 25/200\n",
      "134/134 [==============================] - 0s 960us/step - loss: 2.5594 - accuracy: 0.2463 - val_loss: 2.6313 - val_accuracy: 0.2400\n",
      "Epoch 26/200\n",
      "134/134 [==============================] - 0s 908us/step - loss: 2.5500 - accuracy: 0.2463 - val_loss: 2.6274 - val_accuracy: 0.2400\n",
      "Epoch 27/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.5421 - accuracy: 0.2463 - val_loss: 2.6235 - val_accuracy: 0.2400\n",
      "Epoch 28/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.5353 - accuracy: 0.2463 - val_loss: 2.6195 - val_accuracy: 0.2400\n",
      "Epoch 29/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.5265 - accuracy: 0.2537 - val_loss: 2.6151 - val_accuracy: 0.2400\n",
      "Epoch 30/200\n",
      "134/134 [==============================] - 0s 915us/step - loss: 2.5212 - accuracy: 0.2537 - val_loss: 2.6095 - val_accuracy: 0.2450\n",
      "Epoch 31/200\n",
      "134/134 [==============================] - 0s 841us/step - loss: 2.5090 - accuracy: 0.2687 - val_loss: 2.6024 - val_accuracy: 0.2550\n",
      "Epoch 32/200\n",
      "134/134 [==============================] - 0s 848us/step - loss: 2.4995 - accuracy: 0.2910 - val_loss: 2.5974 - val_accuracy: 0.2700\n",
      "Epoch 33/200\n",
      "134/134 [==============================] - 0s 997us/step - loss: 2.4931 - accuracy: 0.2761 - val_loss: 2.5952 - val_accuracy: 0.2650\n",
      "Epoch 34/200\n",
      "134/134 [==============================] - 0s 841us/step - loss: 2.4874 - accuracy: 0.2836 - val_loss: 2.5915 - val_accuracy: 0.2550\n",
      "Epoch 35/200\n",
      "134/134 [==============================] - 0s 793us/step - loss: 2.4802 - accuracy: 0.2836 - val_loss: 2.5840 - val_accuracy: 0.2650\n",
      "Epoch 36/200\n",
      "134/134 [==============================] - 0s 798us/step - loss: 2.4638 - accuracy: 0.2910 - val_loss: 2.5765 - val_accuracy: 0.2800\n",
      "Epoch 37/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 2.4509 - accuracy: 0.2985 - val_loss: 2.5699 - val_accuracy: 0.2850\n",
      "Epoch 38/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 2.4412 - accuracy: 0.2910 - val_loss: 2.5627 - val_accuracy: 0.2850\n",
      "Epoch 39/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 2.4244 - accuracy: 0.2985 - val_loss: 2.5535 - val_accuracy: 0.2850\n",
      "Epoch 40/200\n",
      "134/134 [==============================] - 0s 927us/step - loss: 2.4159 - accuracy: 0.2910 - val_loss: 2.5434 - val_accuracy: 0.2900\n",
      "Epoch 41/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 2.3928 - accuracy: 0.3134 - val_loss: 2.5329 - val_accuracy: 0.3000\n",
      "Epoch 42/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 2.3824 - accuracy: 0.3209 - val_loss: 2.5214 - val_accuracy: 0.3000\n",
      "Epoch 43/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 2.3678 - accuracy: 0.3134 - val_loss: 2.5094 - val_accuracy: 0.2950\n",
      "Epoch 44/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 2.3480 - accuracy: 0.3060 - val_loss: 2.4978 - val_accuracy: 0.2900\n",
      "Epoch 45/200\n",
      "134/134 [==============================] - 0s 915us/step - loss: 2.3311 - accuracy: 0.3134 - val_loss: 2.4838 - val_accuracy: 0.2950\n",
      "Epoch 46/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 2.3073 - accuracy: 0.3134 - val_loss: 2.4675 - val_accuracy: 0.3000\n",
      "Epoch 47/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 2.2919 - accuracy: 0.3209 - val_loss: 2.4513 - val_accuracy: 0.3100\n",
      "Epoch 48/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 2.2661 - accuracy: 0.3209 - val_loss: 2.4339 - val_accuracy: 0.3200\n",
      "Epoch 49/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 2.2649 - accuracy: 0.3358 - val_loss: 2.4153 - val_accuracy: 0.3300\n",
      "Epoch 50/200\n",
      "134/134 [==============================] - 0s 780us/step - loss: 2.2205 - accuracy: 0.3582 - val_loss: 2.3996 - val_accuracy: 0.3400\n",
      "Epoch 51/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 2.2057 - accuracy: 0.3582 - val_loss: 2.3815 - val_accuracy: 0.3500\n",
      "Epoch 52/200\n",
      "134/134 [==============================] - 0s 975us/step - loss: 2.1834 - accuracy: 0.3731 - val_loss: 2.3630 - val_accuracy: 0.3600\n",
      "Epoch 53/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 2.1611 - accuracy: 0.3806 - val_loss: 2.3445 - val_accuracy: 0.3600\n",
      "Epoch 54/200\n",
      "134/134 [==============================] - 0s 857us/step - loss: 2.1360 - accuracy: 0.3806 - val_loss: 2.3272 - val_accuracy: 0.3600\n",
      "Epoch 55/200\n",
      "134/134 [==============================] - 0s 867us/step - loss: 2.1329 - accuracy: 0.3806 - val_loss: 2.3110 - val_accuracy: 0.3650\n",
      "Epoch 56/200\n",
      "134/134 [==============================] - 0s 841us/step - loss: 2.0928 - accuracy: 0.3955 - val_loss: 2.2976 - val_accuracy: 0.3650\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 811us/step - loss: 2.0709 - accuracy: 0.3955 - val_loss: 2.2899 - val_accuracy: 0.3600\n",
      "Epoch 58/200\n",
      "134/134 [==============================] - 0s 826us/step - loss: 2.0644 - accuracy: 0.3955 - val_loss: 2.2826 - val_accuracy: 0.3650\n",
      "Epoch 59/200\n",
      "134/134 [==============================] - 0s 826us/step - loss: 2.0299 - accuracy: 0.4179 - val_loss: 2.2625 - val_accuracy: 0.3850\n",
      "Epoch 60/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 2.0147 - accuracy: 0.4179 - val_loss: 2.2422 - val_accuracy: 0.3900\n",
      "Epoch 61/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 1.9678 - accuracy: 0.4104 - val_loss: 2.2263 - val_accuracy: 0.3900\n",
      "Epoch 62/200\n",
      "134/134 [==============================] - 0s 841us/step - loss: 1.9718 - accuracy: 0.4030 - val_loss: 2.2144 - val_accuracy: 0.3950\n",
      "Epoch 63/200\n",
      "134/134 [==============================] - 0s 835us/step - loss: 1.9391 - accuracy: 0.4478 - val_loss: 2.2090 - val_accuracy: 0.4000\n",
      "Epoch 64/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 1.9104 - accuracy: 0.4552 - val_loss: 2.1902 - val_accuracy: 0.4100\n",
      "Epoch 65/200\n",
      "134/134 [==============================] - 0s 886us/step - loss: 1.9108 - accuracy: 0.4701 - val_loss: 2.1745 - val_accuracy: 0.4100\n",
      "Epoch 66/200\n",
      "134/134 [==============================] - 0s 960us/step - loss: 1.8537 - accuracy: 0.4851 - val_loss: 2.1595 - val_accuracy: 0.4200\n",
      "Epoch 67/200\n",
      "134/134 [==============================] - 0s 839us/step - loss: 1.8471 - accuracy: 0.4627 - val_loss: 2.1461 - val_accuracy: 0.4250\n",
      "Epoch 68/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 1.8522 - accuracy: 0.4701 - val_loss: 2.1370 - val_accuracy: 0.4200\n",
      "Epoch 69/200\n",
      "134/134 [==============================] - 0s 875us/step - loss: 1.8139 - accuracy: 0.4627 - val_loss: 2.1282 - val_accuracy: 0.4200\n",
      "Epoch 70/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 1.8042 - accuracy: 0.4925 - val_loss: 2.1095 - val_accuracy: 0.4250\n",
      "Epoch 71/200\n",
      "134/134 [==============================] - 0s 826us/step - loss: 1.7753 - accuracy: 0.4776 - val_loss: 2.0900 - val_accuracy: 0.4300\n",
      "Epoch 72/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 1.7566 - accuracy: 0.4627 - val_loss: 2.0753 - val_accuracy: 0.4350\n",
      "Epoch 73/200\n",
      "134/134 [==============================] - 0s 863us/step - loss: 1.7314 - accuracy: 0.5000 - val_loss: 2.0710 - val_accuracy: 0.4450\n",
      "Epoch 74/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 1.7037 - accuracy: 0.5075 - val_loss: 2.0765 - val_accuracy: 0.4500\n",
      "Epoch 75/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 1.6894 - accuracy: 0.4925 - val_loss: 2.0618 - val_accuracy: 0.4550\n",
      "Epoch 76/200\n",
      "134/134 [==============================] - 0s 774us/step - loss: 1.6761 - accuracy: 0.5075 - val_loss: 2.0330 - val_accuracy: 0.4550\n",
      "Epoch 77/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 1.6730 - accuracy: 0.5299 - val_loss: 2.0166 - val_accuracy: 0.4450\n",
      "Epoch 78/200\n",
      "134/134 [==============================] - 0s 767us/step - loss: 1.6462 - accuracy: 0.5299 - val_loss: 2.0034 - val_accuracy: 0.4700\n",
      "Epoch 79/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 1.5977 - accuracy: 0.5597 - val_loss: 1.9937 - val_accuracy: 0.4750\n",
      "Epoch 80/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 1.5981 - accuracy: 0.5448 - val_loss: 1.9897 - val_accuracy: 0.4700\n",
      "Epoch 81/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 1.5787 - accuracy: 0.5672 - val_loss: 1.9751 - val_accuracy: 0.4800\n",
      "Epoch 82/200\n",
      "134/134 [==============================] - 0s 908us/step - loss: 1.5615 - accuracy: 0.5448 - val_loss: 1.9550 - val_accuracy: 0.5000\n",
      "Epoch 83/200\n",
      "134/134 [==============================] - 0s 871us/step - loss: 1.5304 - accuracy: 0.5448 - val_loss: 1.9423 - val_accuracy: 0.5000\n",
      "Epoch 84/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 1.5255 - accuracy: 0.5448 - val_loss: 1.9317 - val_accuracy: 0.5050\n",
      "Epoch 85/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 1.4938 - accuracy: 0.5746 - val_loss: 1.9327 - val_accuracy: 0.5000\n",
      "Epoch 86/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 1.4699 - accuracy: 0.5896 - val_loss: 1.9207 - val_accuracy: 0.5000\n",
      "Epoch 87/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 1.4732 - accuracy: 0.5821 - val_loss: 1.9081 - val_accuracy: 0.4950\n",
      "Epoch 88/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 1.4459 - accuracy: 0.5746 - val_loss: 1.8993 - val_accuracy: 0.4900\n",
      "Epoch 89/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 1.3987 - accuracy: 0.5896 - val_loss: 1.8792 - val_accuracy: 0.5050\n",
      "Epoch 90/200\n",
      "134/134 [==============================] - 0s 799us/step - loss: 1.4148 - accuracy: 0.6045 - val_loss: 1.8675 - val_accuracy: 0.5350\n",
      "Epoch 91/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 1.3641 - accuracy: 0.6343 - val_loss: 1.8662 - val_accuracy: 0.5450\n",
      "Epoch 92/200\n",
      "134/134 [==============================] - 0s 831us/step - loss: 1.3707 - accuracy: 0.6418 - val_loss: 1.8632 - val_accuracy: 0.5200\n",
      "Epoch 93/200\n",
      "134/134 [==============================] - 0s 863us/step - loss: 1.3686 - accuracy: 0.6418 - val_loss: 1.8445 - val_accuracy: 0.5050\n",
      "Epoch 94/200\n",
      "134/134 [==============================] - 0s 927us/step - loss: 1.3316 - accuracy: 0.6194 - val_loss: 1.8243 - val_accuracy: 0.5050\n",
      "Epoch 95/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 1.3191 - accuracy: 0.5970 - val_loss: 1.8148 - val_accuracy: 0.5150\n",
      "Epoch 96/200\n",
      "134/134 [==============================] - 0s 793us/step - loss: 1.3336 - accuracy: 0.5970 - val_loss: 1.7895 - val_accuracy: 0.5450\n",
      "Epoch 97/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 1.2657 - accuracy: 0.6194 - val_loss: 1.7807 - val_accuracy: 0.5600\n",
      "Epoch 98/200\n",
      "134/134 [==============================] - 0s 908us/step - loss: 1.2572 - accuracy: 0.6269 - val_loss: 1.7848 - val_accuracy: 0.5650\n",
      "Epoch 99/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 1.2503 - accuracy: 0.6493 - val_loss: 1.7863 - val_accuracy: 0.5600\n",
      "Epoch 100/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 1.2293 - accuracy: 0.6716 - val_loss: 1.7602 - val_accuracy: 0.5550\n",
      "Epoch 101/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 1.2098 - accuracy: 0.6716 - val_loss: 1.7369 - val_accuracy: 0.5800\n",
      "Epoch 102/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 1.2097 - accuracy: 0.6194 - val_loss: 1.7284 - val_accuracy: 0.5800\n",
      "Epoch 103/200\n",
      "134/134 [==============================] - 0s 767us/step - loss: 1.2097 - accuracy: 0.6493 - val_loss: 1.7167 - val_accuracy: 0.5950\n",
      "Epoch 104/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 1.1288 - accuracy: 0.7015 - val_loss: 1.7261 - val_accuracy: 0.5750\n",
      "Epoch 105/200\n",
      "134/134 [==============================] - 0s 790us/step - loss: 1.1543 - accuracy: 0.7015 - val_loss: 1.7286 - val_accuracy: 0.5750\n",
      "Epoch 106/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 1.1340 - accuracy: 0.7239 - val_loss: 1.6983 - val_accuracy: 0.5700\n",
      "Epoch 107/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 1.1234 - accuracy: 0.7090 - val_loss: 1.6822 - val_accuracy: 0.5950\n",
      "Epoch 108/200\n",
      "134/134 [==============================] - 0s 848us/step - loss: 1.1115 - accuracy: 0.6716 - val_loss: 1.6807 - val_accuracy: 0.5800\n",
      "Epoch 109/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 1.0822 - accuracy: 0.7015 - val_loss: 1.6719 - val_accuracy: 0.6250\n",
      "Epoch 110/200\n",
      "134/134 [==============================] - 0s 826us/step - loss: 1.0672 - accuracy: 0.6940 - val_loss: 1.6717 - val_accuracy: 0.6150\n",
      "Epoch 111/200\n",
      "134/134 [==============================] - 0s 990us/step - loss: 1.0769 - accuracy: 0.7388 - val_loss: 1.6802 - val_accuracy: 0.5900\n",
      "Epoch 112/200\n",
      "134/134 [==============================] - 0s 930us/step - loss: 1.0395 - accuracy: 0.7463 - val_loss: 1.6721 - val_accuracy: 0.6100\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 811us/step - loss: 1.0200 - accuracy: 0.7612 - val_loss: 1.6483 - val_accuracy: 0.6100\n",
      "Epoch 114/200\n",
      "134/134 [==============================] - 0s 848us/step - loss: 0.9987 - accuracy: 0.7761 - val_loss: 1.6289 - val_accuracy: 0.6300\n",
      "Epoch 115/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 1.0000 - accuracy: 0.7761 - val_loss: 1.6182 - val_accuracy: 0.6250\n",
      "Epoch 116/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 0.9969 - accuracy: 0.7239 - val_loss: 1.6180 - val_accuracy: 0.6250\n",
      "Epoch 117/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 0.9533 - accuracy: 0.7388 - val_loss: 1.6177 - val_accuracy: 0.6250\n",
      "Epoch 118/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.9729 - accuracy: 0.7687 - val_loss: 1.6105 - val_accuracy: 0.6200\n",
      "Epoch 119/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.9330 - accuracy: 0.7836 - val_loss: 1.5962 - val_accuracy: 0.6250\n",
      "Epoch 120/200\n",
      "134/134 [==============================] - 0s 960us/step - loss: 0.9502 - accuracy: 0.7761 - val_loss: 1.5827 - val_accuracy: 0.6350\n",
      "Epoch 121/200\n",
      "134/134 [==============================] - 0s 830us/step - loss: 0.9204 - accuracy: 0.7761 - val_loss: 1.5768 - val_accuracy: 0.6350\n",
      "Epoch 122/200\n",
      "134/134 [==============================] - 0s 867us/step - loss: 0.9149 - accuracy: 0.7537 - val_loss: 1.5785 - val_accuracy: 0.6350\n",
      "Epoch 123/200\n",
      "134/134 [==============================] - 0s 889us/step - loss: 0.8836 - accuracy: 0.7910 - val_loss: 1.5757 - val_accuracy: 0.6400\n",
      "Epoch 124/200\n",
      "134/134 [==============================] - 0s 815us/step - loss: 0.8733 - accuracy: 0.8209 - val_loss: 1.5699 - val_accuracy: 0.6350\n",
      "Epoch 125/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 0.8799 - accuracy: 0.7761 - val_loss: 1.5585 - val_accuracy: 0.6300\n",
      "Epoch 126/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 0.8732 - accuracy: 0.7537 - val_loss: 1.5463 - val_accuracy: 0.6400\n",
      "Epoch 127/200\n",
      "134/134 [==============================] - 0s 878us/step - loss: 0.8670 - accuracy: 0.7463 - val_loss: 1.5358 - val_accuracy: 0.6400\n",
      "Epoch 128/200\n",
      "134/134 [==============================] - 0s 826us/step - loss: 0.8510 - accuracy: 0.8134 - val_loss: 1.5358 - val_accuracy: 0.6350\n",
      "Epoch 129/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 0.8585 - accuracy: 0.7761 - val_loss: 1.5399 - val_accuracy: 0.6450\n",
      "Epoch 130/200\n",
      "134/134 [==============================] - 0s 915us/step - loss: 0.8049 - accuracy: 0.8134 - val_loss: 1.5299 - val_accuracy: 0.6450\n",
      "Epoch 131/200\n",
      "134/134 [==============================] - 0s 863us/step - loss: 0.8020 - accuracy: 0.8060 - val_loss: 1.5150 - val_accuracy: 0.6500\n",
      "Epoch 132/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 0.7865 - accuracy: 0.7985 - val_loss: 1.5014 - val_accuracy: 0.6600\n",
      "Epoch 133/200\n",
      "134/134 [==============================] - 0s 871us/step - loss: 0.7771 - accuracy: 0.8134 - val_loss: 1.4972 - val_accuracy: 0.6650\n",
      "Epoch 134/200\n",
      "134/134 [==============================] - 0s 841us/step - loss: 0.7561 - accuracy: 0.8209 - val_loss: 1.4957 - val_accuracy: 0.6600\n",
      "Epoch 135/200\n",
      "134/134 [==============================] - 0s 901us/step - loss: 0.7272 - accuracy: 0.8433 - val_loss: 1.4942 - val_accuracy: 0.6500\n",
      "Epoch 136/200\n",
      "134/134 [==============================] - 0s 912us/step - loss: 0.7359 - accuracy: 0.8433 - val_loss: 1.4921 - val_accuracy: 0.6650\n",
      "Epoch 137/200\n",
      "134/134 [==============================] - 0s 886us/step - loss: 0.7431 - accuracy: 0.8209 - val_loss: 1.4832 - val_accuracy: 0.6650\n",
      "Epoch 138/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 0.7144 - accuracy: 0.8433 - val_loss: 1.4703 - val_accuracy: 0.6900\n",
      "Epoch 139/200\n",
      "134/134 [==============================] - 0s 856us/step - loss: 0.7083 - accuracy: 0.8507 - val_loss: 1.4667 - val_accuracy: 0.6900\n",
      "Epoch 140/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 0.6964 - accuracy: 0.8433 - val_loss: 1.4631 - val_accuracy: 0.6850\n",
      "Epoch 141/200\n",
      "134/134 [==============================] - 0s 878us/step - loss: 0.6679 - accuracy: 0.8433 - val_loss: 1.4574 - val_accuracy: 0.6950\n",
      "Epoch 142/200\n",
      "134/134 [==============================] - 0s 878us/step - loss: 0.6693 - accuracy: 0.8433 - val_loss: 1.4585 - val_accuracy: 0.6950\n",
      "Epoch 143/200\n",
      "134/134 [==============================] - 0s 826us/step - loss: 0.6619 - accuracy: 0.8731 - val_loss: 1.4571 - val_accuracy: 0.6750\n",
      "Epoch 144/200\n",
      "134/134 [==============================] - 0s 908us/step - loss: 0.6559 - accuracy: 0.8806 - val_loss: 1.4540 - val_accuracy: 0.6650\n",
      "Epoch 145/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 0.6437 - accuracy: 0.8657 - val_loss: 1.4556 - val_accuracy: 0.6700\n",
      "Epoch 146/200\n",
      "134/134 [==============================] - 0s 886us/step - loss: 0.6355 - accuracy: 0.8731 - val_loss: 1.4451 - val_accuracy: 0.6800\n",
      "Epoch 147/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.6385 - accuracy: 0.8507 - val_loss: 1.4235 - val_accuracy: 0.6800\n",
      "Epoch 148/200\n",
      "134/134 [==============================] - 0s 871us/step - loss: 0.6096 - accuracy: 0.8433 - val_loss: 1.4102 - val_accuracy: 0.6850\n",
      "Epoch 149/200\n",
      "134/134 [==============================] - 0s 835us/step - loss: 0.6187 - accuracy: 0.8657 - val_loss: 1.4068 - val_accuracy: 0.6900\n",
      "Epoch 150/200\n",
      "134/134 [==============================] - 0s 871us/step - loss: 0.6155 - accuracy: 0.8731 - val_loss: 1.4045 - val_accuracy: 0.6950\n",
      "Epoch 151/200\n",
      "134/134 [==============================] - 0s 863us/step - loss: 0.6135 - accuracy: 0.8806 - val_loss: 1.4039 - val_accuracy: 0.7050\n",
      "Epoch 152/200\n",
      "134/134 [==============================] - 0s 982us/step - loss: 0.5734 - accuracy: 0.9030 - val_loss: 1.4058 - val_accuracy: 0.7050\n",
      "Epoch 153/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 0.6022 - accuracy: 0.8507 - val_loss: 1.4037 - val_accuracy: 0.7100\n",
      "Epoch 154/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 0.5592 - accuracy: 0.8881 - val_loss: 1.3951 - val_accuracy: 0.7100\n",
      "Epoch 155/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 0.5535 - accuracy: 0.8881 - val_loss: 1.3868 - val_accuracy: 0.7050\n",
      "Epoch 156/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.5654 - accuracy: 0.8657 - val_loss: 1.3811 - val_accuracy: 0.7100\n",
      "Epoch 157/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.5462 - accuracy: 0.9104 - val_loss: 1.3765 - val_accuracy: 0.7100\n",
      "Epoch 158/200\n",
      "134/134 [==============================] - 0s 889us/step - loss: 0.5207 - accuracy: 0.8881 - val_loss: 1.3765 - val_accuracy: 0.7150\n",
      "Epoch 159/200\n",
      "134/134 [==============================] - 0s 923us/step - loss: 0.5248 - accuracy: 0.8955 - val_loss: 1.3834 - val_accuracy: 0.7100\n",
      "Epoch 160/200\n",
      "134/134 [==============================] - 0s 855us/step - loss: 0.5339 - accuracy: 0.8955 - val_loss: 1.3862 - val_accuracy: 0.7050\n",
      "Epoch 161/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.9104 - val_loss: 1.3692 - val_accuracy: 0.7100\n",
      "Epoch 162/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 0.5116 - accuracy: 0.9104 - val_loss: 1.3532 - val_accuracy: 0.7200\n",
      "Epoch 163/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 0.4962 - accuracy: 0.8955 - val_loss: 1.3451 - val_accuracy: 0.7250\n",
      "Epoch 164/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 0.4872 - accuracy: 0.9030 - val_loss: 1.3466 - val_accuracy: 0.7150\n",
      "Epoch 165/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 0.4607 - accuracy: 0.9030 - val_loss: 1.3533 - val_accuracy: 0.7100\n",
      "Epoch 166/200\n",
      "134/134 [==============================] - 0s 826us/step - loss: 0.4525 - accuracy: 0.9104 - val_loss: 1.3615 - val_accuracy: 0.7050\n",
      "Epoch 167/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 0.4697 - accuracy: 0.9328 - val_loss: 1.3646 - val_accuracy: 0.7100\n",
      "Epoch 168/200\n",
      "134/134 [==============================] - 0s 796us/step - loss: 0.4729 - accuracy: 0.9104 - val_loss: 1.3577 - val_accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 0.4366 - accuracy: 0.9403 - val_loss: 1.3463 - val_accuracy: 0.7100\n",
      "Epoch 170/200\n",
      "134/134 [==============================] - 0s 901us/step - loss: 0.4287 - accuracy: 0.9254 - val_loss: 1.3413 - val_accuracy: 0.7100\n",
      "Epoch 171/200\n",
      "134/134 [==============================] - 0s 906us/step - loss: 0.4396 - accuracy: 0.9179 - val_loss: 1.3422 - val_accuracy: 0.7200\n",
      "Epoch 172/200\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.9179 - val_loss: 1.3447 - val_accuracy: 0.7200\n",
      "Epoch 173/200\n",
      "134/134 [==============================] - 0s 831us/step - loss: 0.4280 - accuracy: 0.9179 - val_loss: 1.3396 - val_accuracy: 0.7150\n",
      "Epoch 174/200\n",
      "134/134 [==============================] - 0s 807us/step - loss: 0.4233 - accuracy: 0.9254 - val_loss: 1.3277 - val_accuracy: 0.7150\n",
      "Epoch 175/200\n",
      "134/134 [==============================] - 0s 860us/step - loss: 0.4038 - accuracy: 0.9478 - val_loss: 1.3238 - val_accuracy: 0.7200\n",
      "Epoch 176/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 0.4151 - accuracy: 0.9030 - val_loss: 1.3224 - val_accuracy: 0.7200\n",
      "Epoch 177/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 0.3992 - accuracy: 0.9403 - val_loss: 1.3268 - val_accuracy: 0.7250\n",
      "Epoch 178/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 0.3764 - accuracy: 0.9403 - val_loss: 1.3285 - val_accuracy: 0.7200\n",
      "Epoch 179/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 0.3614 - accuracy: 0.9552 - val_loss: 1.3238 - val_accuracy: 0.7150\n",
      "Epoch 180/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 0.3828 - accuracy: 0.9254 - val_loss: 1.3175 - val_accuracy: 0.7250\n",
      "Epoch 181/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 0.3467 - accuracy: 0.9403 - val_loss: 1.3129 - val_accuracy: 0.7300\n",
      "Epoch 182/200\n",
      "134/134 [==============================] - 0s 863us/step - loss: 0.3742 - accuracy: 0.9403 - val_loss: 1.3108 - val_accuracy: 0.7300\n",
      "Epoch 183/200\n",
      "134/134 [==============================] - 0s 804us/step - loss: 0.3661 - accuracy: 0.9478 - val_loss: 1.3090 - val_accuracy: 0.7250\n",
      "Epoch 184/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 0.3572 - accuracy: 0.9552 - val_loss: 1.3140 - val_accuracy: 0.7250\n",
      "Epoch 185/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 0.3540 - accuracy: 0.9552 - val_loss: 1.3187 - val_accuracy: 0.7250\n",
      "Epoch 186/200\n",
      "134/134 [==============================] - 0s 797us/step - loss: 0.3459 - accuracy: 0.9403 - val_loss: 1.3182 - val_accuracy: 0.7250\n",
      "Epoch 187/200\n",
      "134/134 [==============================] - 0s 800us/step - loss: 0.3480 - accuracy: 0.9478 - val_loss: 1.3113 - val_accuracy: 0.7350\n",
      "Epoch 188/200\n",
      "134/134 [==============================] - 0s 830us/step - loss: 0.3330 - accuracy: 0.9627 - val_loss: 1.3031 - val_accuracy: 0.7400\n",
      "Epoch 189/200\n",
      "134/134 [==============================] - 0s 860us/step - loss: 0.3392 - accuracy: 0.9627 - val_loss: 1.2988 - val_accuracy: 0.7350\n",
      "Epoch 190/200\n",
      "134/134 [==============================] - 0s 808us/step - loss: 0.3279 - accuracy: 0.9478 - val_loss: 1.2958 - val_accuracy: 0.7300\n",
      "Epoch 191/200\n",
      "134/134 [==============================] - 0s 789us/step - loss: 0.3411 - accuracy: 0.9478 - val_loss: 1.2963 - val_accuracy: 0.7250\n",
      "Epoch 192/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 0.3173 - accuracy: 0.9552 - val_loss: 1.2948 - val_accuracy: 0.7250\n",
      "Epoch 193/200\n",
      "134/134 [==============================] - 0s 856us/step - loss: 0.3105 - accuracy: 0.9627 - val_loss: 1.2885 - val_accuracy: 0.7350\n",
      "Epoch 194/200\n",
      "134/134 [==============================] - 0s 938us/step - loss: 0.3101 - accuracy: 0.9627 - val_loss: 1.2827 - val_accuracy: 0.7350\n",
      "Epoch 195/200\n",
      "134/134 [==============================] - 0s 811us/step - loss: 0.3280 - accuracy: 0.9403 - val_loss: 1.2784 - val_accuracy: 0.7350\n",
      "Epoch 196/200\n",
      "134/134 [==============================] - 0s 781us/step - loss: 0.3088 - accuracy: 0.9552 - val_loss: 1.2817 - val_accuracy: 0.7300\n",
      "Epoch 197/200\n",
      "134/134 [==============================] - 0s 819us/step - loss: 0.2842 - accuracy: 0.9701 - val_loss: 1.2851 - val_accuracy: 0.7300\n",
      "Epoch 198/200\n",
      "134/134 [==============================] - 0s 919us/step - loss: 0.2788 - accuracy: 0.9776 - val_loss: 1.2833 - val_accuracy: 0.7400\n",
      "Epoch 199/200\n",
      "134/134 [==============================] - 0s 997us/step - loss: 0.2749 - accuracy: 0.9701 - val_loss: 1.2778 - val_accuracy: 0.7500\n",
      "Epoch 200/200\n",
      "134/134 [==============================] - 0s 834us/step - loss: 0.2929 - accuracy: 0.9478 - val_loss: 1.2700 - val_accuracy: 0.7450\n",
      "66/66 [==============================] - 0s 2ms/step\n",
      "\n",
      " Test Accuracy: 0.2424\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=100, epochs=200, validation_data=(x_test, y_test))\n",
    "print('\\n Test Accuracy: {0:0.4f}'.format(model.evaluate(x_te, y_te)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2727272727272727"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "fp = codecs.open('yang.txt', 'r')\n",
    "lines = fp.readlines()  # 총 200줄의 내용\n",
    "all_sen2 = []\n",
    "\n",
    "for line in lines:\n",
    "    line2 = line.split()\n",
    "    sen_idx = [text for text in line2 if text in word_dic]\n",
    "    all_sen2.append(' '.join(sen_idx))\n",
    "# print(all_sen2)\n",
    "\n",
    "X = pd.DataFrame(all_sen2, columns=['title'])\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(\n",
    "    X['title'], Y_train_all, test_size=0.33, random_state=0\n",
    ")\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "training_data = count_vector.fit_transform(x_tr)\n",
    "testing_data = count_vector.transform(x_te)\n",
    "print(type(training_data))\n",
    "print(y_train)\n",
    "\n",
    "gnb = MultinomialNB()\n",
    "gnb.fit(training_data, y_tr)\n",
    "prediction = gnb.predict(testing_data)\n",
    "metrics.accuracy_score(prediction, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
